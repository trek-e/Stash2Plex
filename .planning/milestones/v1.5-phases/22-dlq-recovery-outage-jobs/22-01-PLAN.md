---
phase: 22-dlq-recovery-outage-jobs
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - sync_queue/dlq_recovery.py
  - tests/sync_queue/test_dlq_recovery.py
autonomous: true

must_haves:
  truths:
    - "DLQ entries from outage window can be queried by time range and error type"
    - "Only PlexServerDown errors are recovered by default (conservative)"
    - "Recovery skips entries already in queue (deduplication)"
    - "Recovery skips entries for scenes deleted from Stash"
    - "Recovery is blocked when Plex is unhealthy (pre-flight gate)"
    - "Recovery is idempotent (safe to run multiple times)"
  artifacts:
    - path: "sync_queue/dlq_recovery.py"
      provides: "DLQ recovery operations module"
      exports: ["SAFE_RETRY_ERROR_TYPES", "OPTIONAL_RETRY_ERROR_TYPES", "PERMANENT_ERROR_TYPES", "get_error_types_for_recovery", "get_outage_dlq_entries", "recover_outage_jobs", "RecoveryResult"]
    - path: "tests/sync_queue/test_dlq_recovery.py"
      provides: "Comprehensive tests for DLQ recovery"
      min_lines: 150
  key_links:
    - from: "sync_queue/dlq_recovery.py"
      to: "sync_queue/dlq.py"
      via: "DeadLetterQueue._get_connection() for SQLite queries"
      pattern: "dlq\\._get_connection"
    - from: "sync_queue/dlq_recovery.py"
      to: "sync_queue/operations.py"
      via: "get_queued_scene_ids() for deduplication, enqueue() for re-queue"
      pattern: "get_queued_scene_ids|enqueue"
    - from: "sync_queue/dlq_recovery.py"
      to: "plex/health.py"
      via: "check_plex_health() for pre-flight validation"
      pattern: "check_plex_health"
---

<objective>
TDD: Create DLQ recovery module with error classification, time-windowed queries, and idempotent recovery.

Purpose: Enable selective recovery of DLQ entries that failed during Plex outage windows, with conservative defaults (PlexServerDown only), deduplication, and three-gate validation (Plex health, dedup, scene existence).

Output: `sync_queue/dlq_recovery.py` with full test coverage.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-dlq-recovery-outage-jobs/22-RESEARCH.md

@sync_queue/dlq.py
@sync_queue/operations.py
@plex/health.py
@plex/exceptions.py
@worker/outage_history.py
@tests/sync_queue/test_dlq.py
</context>

<feature>
  <name>DLQ Recovery Module</name>
  <files>sync_queue/dlq_recovery.py, tests/sync_queue/test_dlq_recovery.py</files>
  <behavior>
    Three components with testable I/O:

    1. Error Type Classification Constants:
       - SAFE_RETRY_ERROR_TYPES = ["PlexServerDown"] (default recovery set)
       - OPTIONAL_RETRY_ERROR_TYPES = ["PlexTemporaryError", "PlexNotFound"]
       - PERMANENT_ERROR_TYPES = ["PlexPermanentError", "PlexAuthError", "PlexPermissionError"]
       - get_error_types_for_recovery(include_optional=False) -> List[str]
         - False -> ["PlexServerDown"]
         - True -> ["PlexServerDown", "PlexTemporaryError", "PlexNotFound"]

    2. get_outage_dlq_entries(dlq, start_time, end_time, error_types) -> List[dict]:
       - Queries DLQ SQLite database for entries where:
         - failed_at is within [start_time, end_time] (inclusive)
         - error_type is in error_types list
       - CRITICAL TIMESTAMP FORMAT MISMATCH: DLQ `failed_at` column uses
         `TIMESTAMP DEFAULT CURRENT_TIMESTAMP` which stores as SQLite text
         format ("2026-02-15 19:17:49"), while OutageHistory timestamps are
         Unix floats. The query MUST convert start_time/end_time floats
         using `datetime(?, 'unixepoch')` for comparison in the WHERE clause.
         Example: `WHERE failed_at >= datetime(?, 'unixepoch') AND failed_at <= datetime(?, 'unixepoch')`
       - Returns list of dicts with: id, scene_id, error_type, error_message, failed_at, job_data
       - Results ordered by failed_at ASC (oldest first)
       - Empty list if no matches

    3. recover_outage_jobs(dlq_entries, queue, stash, plex_client, data_dir) -> RecoveryResult:
       - RecoveryResult dataclass with fields:
         total_dlq_entries, recovered, skipped_already_queued,
         skipped_plex_down, skipped_scene_missing, failed,
         recovered_scene_ids (List[int])
       - Gate 1: check_plex_health(plex_client) - if unhealthy, set
         skipped_plex_down=total and return immediately
       - Gate 2: get_queued_scene_ids(queue_path) for dedup set
       - For each entry:
         - Skip if scene_id in already_queued set -> skipped_already_queued++
         - Gate 3: stash.find_scene(scene_id) -> if None/missing -> skipped_scene_missing++
         - Deserialize job_data (stored as BLOB via existing DLQ pattern),
           call enqueue(queue, scene_id, update_type, data)
         - Add scene_id to already_queued set (in-memory batch dedup)
         - recovered++, append to recovered_scene_ids
         - On exception: failed++
       - Returns RecoveryResult

    Test cases (RED phase - write these first):
      Error classification:
        get_error_types_for_recovery(False) -> ["PlexServerDown"]
        get_error_types_for_recovery(True) -> ["PlexServerDown", "PlexTemporaryError", "PlexNotFound"]
        PlexAuthError NOT in any safe list

      get_outage_dlq_entries:
        Empty DLQ -> []
        DLQ with entries outside time window -> []
        DLQ with entries inside window but wrong error type -> []
        DLQ with matching entries -> returns them ordered by failed_at ASC
        Multiple error types in filter -> returns all matching types
        Boundary: entry at exact start_time included
        Boundary: entry at exact end_time included

      recover_outage_jobs:
        Plex unhealthy -> skipped_plex_down = total, recovered = 0
        All entries already queued -> skipped_already_queued = total
        Scene missing from Stash -> skipped_scene_missing++
        Successful recovery -> recovered++, scene_id in recovered_scene_ids
        Duplicate scene_id in batch -> second one skipped (in-memory dedup)
        Mixed results -> correct counts for each category
        Idempotent: run twice with same entries -> second run skips all (already queued)
  </behavior>
  <implementation>
    Create sync_queue/dlq_recovery.py:

    - Import typing, dataclasses, os, sqlite3 (for Row factory)
    - Define error type constant lists (strings matching type(error).__name__ from plex/exceptions.py)
    - get_error_types_for_recovery(): simple list copy + extend
    - RecoveryResult: @dataclass with int fields, default 0, recovered_scene_ids list with __post_init__
    - get_outage_dlq_entries(): use dlq._get_connection() to run SQL query:
        SELECT id, scene_id, error_type, error_message, failed_at, job_data
        FROM dead_letters
        WHERE failed_at >= datetime(?, 'unixepoch')
          AND failed_at <= datetime(?, 'unixepoch')
          AND error_type IN (?,?,...)
        ORDER BY failed_at ASC
      Use conn.row_factory = sqlite3.Row, return [dict(row) for row in cursor]
    - recover_outage_jobs(): three-gate validation as described above
      - Uses check_plex_health from plex.health
      - Uses get_queued_scene_ids from sync_queue.operations
      - Uses enqueue from sync_queue.operations
      - Deserializes job_data blob per existing DLQ pattern (see dlq.get_by_id for reference)
  </implementation>
</feature>

<verification>
```bash
# Run tests (should all pass after GREEN phase)
cd /Users/trekkie/projects/PlexSync && python -m pytest tests/sync_queue/test_dlq_recovery.py -v

# Run full suite to check no regressions
cd /Users/trekkie/projects/PlexSync && python -m pytest --tb=short -q

# Coverage check
cd /Users/trekkie/projects/PlexSync && python -m pytest --cov=sync_queue.dlq_recovery tests/sync_queue/test_dlq_recovery.py --cov-report=term-missing
```
</verification>

<success_criteria>
- sync_queue/dlq_recovery.py exists with all exports
- All test cases pass (error classification, time-windowed queries, recovery with gates)
- get_outage_dlq_entries correctly handles the text/float timestamp mismatch
- recover_outage_jobs is demonstrably idempotent in tests
- No regressions in full test suite (1182+ tests pass)
- Coverage for dlq_recovery.py above 90%
</success_criteria>

<output>
After completion, create `.planning/phases/22-dlq-recovery-outage-jobs/22-01-SUMMARY.md`
</output>
