---
phase: 08-observability-improvements
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - worker/processor.py
  - tests/unit/worker/test_processor.py
  - tests/integration/test_sync_workflow.py
autonomous: true

must_haves:
  truths:
    - "Processor logs batch summary every 10 jobs"
    - "Batch summary includes success/fail counts and timing"
    - "Batch summary includes DLQ breakdown by error type"
    - "Stats are tracked across all job outcomes (success, failure, dlq)"
    - "JSON batch summary is logged at INFO level"
  artifacts:
    - path: "worker/processor.py"
      provides: "Stats-integrated job processing with batch logging"
      contains: "_log_batch_summary"
  key_links:
    - from: "worker/processor.py"
      to: "worker/stats.py"
      via: "SyncStats import and tracking calls"
      pattern: "from worker\\.stats import SyncStats"
    - from: "worker/processor.py"
      to: "sync_queue/dlq.py"
      via: "get_error_summary call in batch logging"
      pattern: "get_error_summary"
---

<objective>
Integrate SyncStats into processor and implement batch summary logging.

Purpose: Complete the observability improvements by tracking stats during job processing and logging periodic summaries that enable diagnosis of sync issues from logs alone.

Output:
- Enhanced `worker/processor.py` with stats tracking and batch summary logging
- Tests verifying stats integration and logging behavior
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-observability-improvements/08-RESEARCH.md
@.planning/phases/08-observability-improvements/08-01-SUMMARY.md

# Files to modify
@worker/processor.py
@worker/stats.py
@sync_queue/dlq.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate SyncStats into processor</name>
  <files>worker/processor.py</files>
  <action>
Modify `worker/processor.py` to track statistics during job processing:

**Add to SyncWorker.__init__:**
```python
# Import at top of file
from worker.stats import SyncStats

# In __init__:
self._stats = SyncStats()

# If data_dir is set, try to load existing stats
if data_dir is not None:
    stats_path = os.path.join(data_dir, 'stats.json')
    self._stats = SyncStats.load_from_file(stats_path)
```

**Track success in _worker_loop (after ack_job, before periodic logging):**
```python
# Get elapsed time (already tracked in _process_job, but we need it here)
# Add timing around _process_job call
_job_start = time.perf_counter()
self._process_job(item)
_job_elapsed = time.perf_counter() - _job_start

# Record success with confidence
# Determine confidence from the match (need to expose this from _process_job)
self._stats.record_success(_job_elapsed, confidence='high')  # Default to high for now
```

**Note:** To track match confidence properly, modify `_process_job` to return the confidence level:
- Change `_process_job` signature to return `Optional[str]` (confidence level or None)
- Return 'high' for single match, 'low' for multiple matches
- In worker loop, capture return value and pass to record_success

**Track failure in TransientError handler:**
```python
self._stats.record_failure(type(e).__name__, _job_elapsed, to_dlq=False)
# If moving to DLQ:
self._stats.record_failure(type(e).__name__, _job_elapsed, to_dlq=True)
```

**Track failure in PermanentError handler:**
```python
self._stats.record_failure(type(e).__name__, _job_elapsed, to_dlq=True)
```

**Save stats periodically (in the batch logging section):**
```python
if self.data_dir is not None:
    stats_path = os.path.join(self.data_dir, 'stats.json')
    self._stats.save_to_file(stats_path)
```
  </action>
  <verify>
Run: `python -c "from worker.processor import SyncWorker; print('Import OK')"`
Import succeeds. Check that _stats attribute is initialized.
  </verify>
  <done>
SyncWorker tracks stats for every job processed (success, failure, timing, confidence). Stats are loaded on startup and saved periodically.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement batch summary logging with DLQ breakdown</name>
  <files>worker/processor.py, tests/unit/worker/test_processor.py</files>
  <action>
Add `_log_batch_summary()` method to SyncWorker and integrate into periodic logging:

**Add method to SyncWorker:**
```python
def _log_batch_summary(self):
    """Log periodic summary of sync operations with JSON stats."""
    import json

    stats = self._stats

    # Human-readable summary line
    log_info(
        f"Sync summary: {stats.jobs_succeeded}/{stats.jobs_processed} succeeded "
        f"({stats.success_rate:.1f}%), avg {stats.avg_processing_time*1000:.0f}ms, "
        f"confidence: {stats.high_confidence_matches} high / {stats.low_confidence_matches} low"
    )

    # JSON batch summary for machine parsing
    stats_dict = {
        "processed": stats.jobs_processed,
        "succeeded": stats.jobs_succeeded,
        "failed": stats.jobs_failed,
        "to_dlq": stats.jobs_to_dlq,
        "success_rate": f"{stats.success_rate:.1f}%",
        "avg_time_ms": int(stats.avg_processing_time * 1000),
        "high_confidence": stats.high_confidence_matches,
        "low_confidence": stats.low_confidence_matches,
        "errors_by_type": stats.errors_by_type,
    }
    log_info(f"Stats: {json.dumps(stats_dict)}")

    # DLQ summary if items present (using new get_error_summary method)
    dlq_summary = self.dlq.get_error_summary()
    if dlq_summary:
        total = sum(dlq_summary.values())
        breakdown = ", ".join(f"{count} {err_type}" for err_type, count in dlq_summary.items())
        log_warn(f"DLQ contains {total} items: {breakdown}")
```

**Replace `_log_dlq_status()` call with `_log_batch_summary()` in periodic logging:**
```python
# In _worker_loop, in the periodic logging section:
self._jobs_since_dlq_log += 1
if self._jobs_since_dlq_log >= self._dlq_log_interval:
    self._log_batch_summary()  # This now includes DLQ status
    self._log_cache_stats()
    self._jobs_since_dlq_log = 0

    # Save stats
    if self.data_dir is not None:
        stats_path = os.path.join(self.data_dir, 'stats.json')
        self._stats.save_to_file(stats_path)
```

**Keep `_log_dlq_status()` for startup logging** (it provides more detail for initial status).

**Add/update tests in `tests/unit/worker/test_processor.py`:**
- Test that _stats is initialized on worker creation
- Test that stats are loaded from file if data_dir is set
- Test that record_success is called on successful job
- Test that record_failure is called on TransientError
- Test that record_failure is called on PermanentError with to_dlq=True
- Test _log_batch_summary produces expected log output (mock log_info/log_warn)

Use existing test patterns - the file has fixtures for mocking the processor dependencies.
  </action>
  <verify>
Run: `pytest tests/unit/worker/test_processor.py -v -k stats`
New stats-related tests pass.
Run: `pytest tests/unit/worker/ -v`
All worker tests pass.
  </verify>
  <done>
Processor logs batch summary every 10 jobs with: success/fail counts, timing, confidence breakdown, errors by type, and DLQ breakdown. Stats are persisted to `{data_dir}/stats.json`.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add integration tests for observability</name>
  <files>tests/integration/test_sync_workflow.py</files>
  <action>
Add integration tests to verify end-to-end observability behavior:

**Add to `tests/integration/test_sync_workflow.py`:**

```python
class TestObservabilityIntegration:
    """Integration tests for observability features."""

    def test_stats_tracked_on_successful_sync(self, integration_worker, sample_job, mock_plex_client):
        """Verify stats are updated after successful job processing."""
        # Process a job
        # Assert _stats.jobs_processed == 1
        # Assert _stats.jobs_succeeded == 1
        # Assert _stats.total_processing_time > 0

    def test_stats_tracked_on_failed_sync(self, integration_worker, sample_job, mock_plex_client):
        """Verify stats are updated after failed job processing."""
        # Configure mock to raise TransientError
        # Process job (will fail)
        # Assert _stats.jobs_failed >= 1
        # Assert _stats.errors_by_type has the error type

    def test_stats_persisted_to_file(self, integration_worker, sample_job, mock_plex_client, tmp_path):
        """Verify stats are saved to JSON file."""
        # Set data_dir to tmp_path
        # Process jobs
        # Trigger batch logging (process 10 jobs or call _log_batch_summary directly)
        # Assert stats.json exists in tmp_path
        # Assert file contains expected stats

    def test_dlq_error_summary_in_batch_log(self, integration_worker, sample_job, mock_plex_client, tmp_path, capsys):
        """Verify DLQ summary appears in batch logs."""
        # Add items to DLQ with different error types
        # Call _log_batch_summary
        # Assert stderr contains "DLQ contains X items: N ErrorType1, M ErrorType2"
```

Use existing integration test patterns and fixtures from the file.
  </action>
  <verify>
Run: `pytest tests/integration/test_sync_workflow.py -v -k Observability`
All observability integration tests pass.
  </verify>
  <done>
Integration tests verify stats tracking, persistence, and DLQ summary logging work end-to-end.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/unit/worker/test_processor.py -v` - All processor tests pass
2. `pytest tests/unit/worker/test_stats.py -v` - All stats tests pass
3. `pytest tests/integration/test_sync_workflow.py -v -k Observability` - Integration tests pass
4. Manual check: Process 10+ jobs, verify logs show batch summary with stats and DLQ breakdown
</verification>

<success_criteria>
- SyncWorker._stats tracks all job outcomes
- Batch summary logged every 10 jobs at INFO level
- Batch summary includes JSON-formatted stats
- DLQ breakdown by error type shown in batch summary
- Stats persisted to `{data_dir}/stats.json`
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-observability-improvements/08-02-SUMMARY.md`
</output>
