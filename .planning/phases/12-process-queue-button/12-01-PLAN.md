---
phase: 12-process-queue-button
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Stash2Plex.py
  - Stash2Plex.yml
autonomous: true

must_haves:
  truths:
    - "User can trigger 'Process Queue' from Stash Tasks menu"
    - "Queue processing continues until queue is empty (no timeout limit)"
    - "User sees progress percentage in Stash task UI"
    - "User sees status messages in Stash logs during processing"
    - "Circuit breaker is respected (stops if Plex unavailable)"
    - "Failed items move to DLQ with proper error classification"
  artifacts:
    - path: "Stash2Plex.py"
      provides: "handle_process_queue function with foreground processing loop"
      contains: "def handle_process_queue"
    - path: "Stash2Plex.yml"
      provides: "Process Queue task definition"
      contains: "mode: process_queue"
  key_links:
    - from: "Stash2Plex.py:handle_task"
      to: "handle_process_queue"
      via: "mode dispatch"
      pattern: "mode == 'process_queue'"
    - from: "handle_process_queue"
      to: "SyncWorker._process_job"
      via: "job processing loop"
      pattern: "worker._process_job"
    - from: "handle_process_queue"
      to: "log_progress"
      via: "Stash UI progress updates"
      pattern: "log_progress\\("
---

<objective>
Add "Process Queue" task that processes all pending items until empty with progress feedback.

Purpose: Users need a way to process large queues that timeout during normal operation. The existing daemon worker is limited by Stash plugin timeout. This task runs processing in the foreground until the queue is empty, not bound by timeout limits.

Output: New Stash task "Process Queue" that:
- Processes all pending items until queue is empty
- Reports progress via log_progress() to Stash UI
- Respects circuit breaker state
- Handles errors with retry/DLQ pattern
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-process-queue-button/12-RESEARCH.md
@.planning/phases/11-queue-management-ui/11-01-SUMMARY.md

# Key source files
@Stash2Plex.py
@Stash2Plex.yml
@worker/processor.py
@process_queue.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add handle_process_queue function</name>
  <files>Stash2Plex.py</files>
  <action>
Add `handle_process_queue()` function before `handle_task()` (after `handle_purge_dlq`).

Implementation pattern (adapted from 12-RESEARCH.md and process_queue.py):

```python
def handle_process_queue():
    """
    Process all pending queue items until empty.

    Runs in foreground (not daemon thread), processing until queue is empty.
    Reports progress via log_progress() for Stash UI visibility.
    Respects circuit breaker - stops if Plex becomes unavailable.
    """
    global config

    try:
        data_dir = get_plugin_data_dir()
        queue_path = os.path.join(data_dir, 'queue')

        # Check initial queue state
        from sync_queue.operations import get_stats, get_pending, ack_job, fail_job
        stats = get_stats(queue_path)
        total = stats['pending'] + stats['in_progress']

        if total == 0:
            log_info("Queue is empty - nothing to process")
            return

        log_info(f"Starting batch processing of {total} items...")
        log_progress(0)

        # Initialize infrastructure
        from sync_queue.manager import QueueManager
        from sync_queue.dlq import DeadLetterQueue
        from worker.processor import SyncWorker, TransientError, PermanentError

        # Configure device identity before Plex operations
        configure_plex_device_identity(data_dir)

        queue_manager_local = QueueManager(data_dir)
        queue = queue_manager_local.get_queue()
        dlq_local = DeadLetterQueue(data_dir)

        # Create worker instance (handles Plex client, caches, circuit breaker)
        worker_local = SyncWorker(queue, dlq_local, config, data_dir=data_dir)

        processed = 0
        failed = 0
        start_time = time.time()
        last_progress_time = start_time

        while True:
            # Check circuit breaker before processing
            if not worker_local.circuit_breaker.can_execute():
                log_warn("Circuit breaker OPEN - Plex may be unavailable")
                log_info(f"Processed {processed} items before circuit break")
                break

            # Get next job (1 second timeout to check for empty queue)
            job = get_pending(queue, timeout=1)
            if job is None:
                break  # Queue is empty

            scene_id = job.get('scene_id', '?')
            retry_count = job.get('retry_count', 0)

            try:
                worker_local._process_job(job)
                ack_job(queue, job)
                worker_local.circuit_breaker.record_success()
                processed += 1

            except TransientError as e:
                worker_local.circuit_breaker.record_failure()
                job = worker_local._prepare_for_retry(job, e)
                max_retries = worker_local._get_max_retries_for_error(e)

                if job.get('retry_count', 0) >= max_retries:
                    log_warn(f"Scene {scene_id}: max retries exceeded, moving to DLQ")
                    fail_job(queue, job)
                    dlq_local.add(job, e, job.get('retry_count', 0))
                    failed += 1
                else:
                    # Re-queue for retry
                    worker_local._requeue_with_metadata(job)
                    log_debug(f"Scene {scene_id}: transient error, will retry")

            except PermanentError as e:
                log_warn(f"Scene {scene_id}: permanent error: {e}")
                fail_job(queue, job)
                dlq_local.add(job, e, retry_count)
                failed += 1

            except Exception as e:
                log_error(f"Scene {scene_id}: unexpected error: {e}")
                fail_job(queue, job)
                dlq_local.add(job, e, retry_count)
                failed += 1

            # Report progress every 5 items or every 10 seconds
            now = time.time()
            if processed % 5 == 0 or (now - last_progress_time) >= 10:
                progress = (processed / total) * 100 if total > 0 else 100
                remaining = queue.size
                log_progress(progress)
                elapsed = now - start_time
                rate = processed / elapsed if elapsed > 0 else 0
                log_info(f"Progress: {processed}/{total} ({progress:.0f}%), "
                        f"{remaining} remaining, {rate:.1f} items/sec")
                last_progress_time = now

        # Final summary
        elapsed = time.time() - start_time
        log_progress(100)
        log_info(f"Batch processing complete: {processed} succeeded, {failed} failed in {elapsed:.1f}s")

        # Show DLQ count if items were added
        if failed > 0:
            dlq_count = dlq_local.get_count()
            log_warn(f"DLQ contains {dlq_count} items requiring review")

    except Exception as e:
        log_error(f"Process queue error: {e}")
        import traceback
        traceback.print_exc()
```

Key points:
- Use `time` module (already imported at top of file? Check - add `import time` if needed)
- Use local variables (queue_manager_local, dlq_local, worker_local) to avoid conflicts with globals
- Circuit breaker check BEFORE each job
- Progress reporting every 5 items OR every 10 seconds (whichever comes first)
- Handle TransientError, PermanentError, and generic Exception
- Final 100% progress on completion
  </action>
  <verify>
Run `python -c "import Stash2Plex"` - should import without error.
Run `grep -n "def handle_process_queue" Stash2Plex.py` - should find the function.
  </verify>
  <done>
handle_process_queue function exists and handles:
- Empty queue check
- Foreground processing loop
- Circuit breaker respect
- Progress reporting via log_progress()
- Error classification (Transient/Permanent/Unknown)
- DLQ integration
  </done>
</task>

<task type="auto">
  <name>Task 2: Add task definition and wire dispatcher</name>
  <files>Stash2Plex.yml, Stash2Plex.py</files>
  <action>
**Part A: Add task to Stash2Plex.yml**

Add new task definition after the existing queue management tasks (after "Purge Old DLQ Entries"):

```yaml
  - name: Process Queue
    description: Process all pending queue items until empty (resumes stuck queues)
    defaultArgs:
      mode: process_queue
```

**Part B: Wire dispatcher in handle_task()**

In Stash2Plex.py, update handle_task() to dispatch process_queue mode.

Add BEFORE the "# Sync tasks require Stash connection" comment:

```python
    elif mode == 'process_queue':
        handle_process_queue()
        return
```

This should be added after the `purge_dlq` branch:
```python
    elif mode == 'purge_dlq':
        days = task_args.get('days', 30)
        handle_purge_dlq(days)
        return
    elif mode == 'process_queue':  # ADD THIS
        handle_process_queue()      # ADD THIS
        return                       # ADD THIS

    # Sync tasks require Stash connection
```
  </action>
  <verify>
Run `python -c "import yaml; yaml.safe_load(open('Stash2Plex.yml'))"` - should parse without error.
Run `grep -c "mode: process_queue" Stash2Plex.yml` - should return 1.
Run `grep -c "handle_process_queue" Stash2Plex.py` - should return at least 2 (def + call).
Count tasks: `grep -c "^  - name:" Stash2Plex.yml` - should return 7 (was 6, now +1).
  </verify>
  <done>
- Stash2Plex.yml has 7 tasks (6 original + 1 new)
- handle_task() dispatches process_queue mode to handle_process_queue()
- Task appears in Stash UI menu
  </done>
</task>

<task type="auto">
  <name>Task 3: Add missing import and verify integration</name>
  <files>Stash2Plex.py</files>
  <action>
**Check and add `time` import if needed:**

Look at the imports at the top of Stash2Plex.py. If `time` is not imported, add it:

```python
import os
import sys
import json
import time  # Add if missing
```

**Verify full integration:**

Run a quick syntax check and grep for key patterns:
- Function defined: `def handle_process_queue`
- Function called: `handle_process_queue()`
- Progress reporting: `log_progress(`
- Circuit breaker check: `can_execute()`
- Error handling: `TransientError`, `PermanentError`

Run Python import to verify no syntax errors.
  </action>
  <verify>
Run `python -c "import Stash2Plex; print('OK')"` - should print OK.
Run `python -c "import time; print(time.time())"` - should print timestamp (confirms time available).
Run full verification:
```bash
grep -c "import time" Stash2Plex.py && \
grep -c "def handle_process_queue" Stash2Plex.py && \
grep -c "handle_process_queue()" Stash2Plex.py && \
grep -c "log_progress" Stash2Plex.py && \
grep -c "can_execute" Stash2Plex.py
```
All should return >= 1.
  </verify>
  <done>
- time module imported
- All integration points verified
- Python import succeeds without errors
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **File structure check:**
   - Stash2Plex.py contains handle_process_queue function
   - Stash2Plex.yml contains 7 tasks
   - Mode dispatch includes process_queue

2. **Import verification:**
   ```bash
   cd /Users/trekkie/projects/PlexSync && python -c "import Stash2Plex; print('Import OK')"
   ```

3. **Function signature check:**
   ```bash
   grep -A 5 "def handle_process_queue" Stash2Plex.py
   ```
   Should show function with docstring mentioning foreground processing.

4. **Progress reporting check:**
   ```bash
   grep "log_progress" Stash2Plex.py | wc -l
   ```
   Should be >= 3 (definition + at least 2 calls in handle_process_queue).

5. **Requirements coverage:**
   - PROC-01: Task in Stash2Plex.yml with mode: process_queue
   - PROC-02: While loop processes until queue empty
   - PROC-03: log_progress() calls for Stash UI
   - PROC-04: Foreground processing not limited by daemon timeout
   - PROC-05: While loop exits only when job is None (queue empty)
   - PROC-06: Batch processing with error handling and DLQ
</verification>

<success_criteria>
- [ ] handle_process_queue() function exists in Stash2Plex.py
- [ ] Function processes until queue empty (while True loop with break on None)
- [ ] Progress reported via log_progress() every 5 items or 10 seconds
- [ ] Circuit breaker checked before each job (can_execute())
- [ ] Errors handled with TransientError/PermanentError/generic classification
- [ ] Failed items moved to DLQ
- [ ] Task definition in Stash2Plex.yml with mode: process_queue
- [ ] handle_task() dispatches to handle_process_queue() for mode='process_queue'
- [ ] Python import succeeds without errors
- [ ] YAML parses without errors
</success_criteria>

<output>
After completion, create `.planning/phases/12-process-queue-button/12-01-SUMMARY.md`
</output>
