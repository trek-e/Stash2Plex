---
phase: 02-core-unit-tests
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/validation/test_metadata.py
  - tests/validation/test_config.py
  - tests/validation/test_sanitizers.py
  - tests/validation/test_errors.py
autonomous: true

must_haves:
  truths:
    - "SyncMetadata validates required fields (scene_id, title)"
    - "SyncMetadata rejects invalid inputs with clear errors"
    - "PlexSyncConfig validates URL and token requirements"
    - "sanitize_for_plex removes control characters and normalizes text"
    - "validate_metadata helper returns tuple of (model, error)"
    - "classify_http_error correctly classifies transient vs permanent HTTP codes"
    - "classify_exception correctly classifies exception types for retry/DLQ routing"
  artifacts:
    - path: "tests/validation/test_metadata.py"
      provides: "SyncMetadata validation tests"
      contains: "class TestSyncMetadata"
    - path: "tests/validation/test_config.py"
      provides: "PlexSyncConfig validation tests"
      contains: "class TestPlexSyncConfig"
    - path: "tests/validation/test_sanitizers.py"
      provides: "Sanitizer function tests"
      contains: "def test_sanitize_for_plex"
    - path: "tests/validation/test_errors.py"
      provides: "Error classification tests"
      contains: "class TestClassifyHttpError"
  key_links:
    - from: "tests/validation/test_metadata.py"
      to: "validation/metadata.py"
      via: "imports SyncMetadata, validate_metadata"
      pattern: "from validation.metadata import"
    - from: "tests/validation/test_config.py"
      to: "validation/config.py"
      via: "imports PlexSyncConfig, validate_config"
      pattern: "from validation.config import"
    - from: "tests/validation/test_errors.py"
      to: "validation/errors.py"
      via: "imports classify_http_error, classify_exception"
      pattern: "from validation.errors import"
---

<objective>
Create comprehensive unit tests for the validation module (SyncMetadata, PlexSyncConfig, sanitizers, error classification).

Purpose: Test Pydantic model validation with parametrized edge cases, plus error classification for retry/DLQ routing.
Output: Four test files covering all validation scenarios with >80% coverage.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-core-unit-tests/02-RESEARCH.md
@validation/metadata.py
@validation/config.py
@validation/sanitizers.py
@validation/errors.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: SyncMetadata and validate_metadata Tests</name>
  <files>tests/validation/test_metadata.py</files>
  <action>
Create test_metadata.py with TestSyncMetadata class using @pytest.mark.parametrize extensively:

Required field tests:
- test_valid_minimal_metadata: scene_id=1, title="Test" creates valid model
- test_scene_id_must_be_positive: parametrize [0, -1, -100], expect ValidationError
- test_scene_id_required: omit scene_id, expect error
- test_title_required: omit title, expect error
- test_title_cannot_be_empty: title="", expect error
- test_title_whitespace_only_rejected: title="   ", expect error (sanitization makes empty)

Optional field tests:
- test_all_optional_fields_accepted: Include details, date, rating100, studio, performers, tags
- test_details_max_length: 10001 chars rejected, 10000 chars accepted
- test_rating100_range: parametrize [(0, True), (50, True), (100, True), (-1, False), (101, False)]
- test_performers_list_accepted: performers=["A", "B"] works
- test_tags_list_accepted: tags=["X", "Y"] works

Sanitization tests:
- test_title_control_chars_removed: title with \x00\x1f sanitized
- test_title_smart_quotes_converted: title with unicode quotes normalized
- test_details_sanitized: details with control chars cleaned
- test_studio_sanitized: studio name cleaned
- test_performers_sanitized: list items cleaned
- test_tags_sanitized: list items cleaned

validate_metadata helper tests:
- test_validate_metadata_success_returns_model: valid data returns (model, None)
- test_validate_metadata_failure_returns_error: invalid data returns (None, error_string)
- test_validate_metadata_error_includes_field_name: error message mentions failing field

Use sample_metadata_dict fixture from conftest.py as base for valid data.
  </action>
  <verify>pytest tests/validation/test_metadata.py -v</verify>
  <done>All SyncMetadata tests pass with parametrized validation edge cases</done>
</task>

<task type="auto">
  <name>Task 2: PlexSyncConfig and validate_config Tests</name>
  <files>tests/validation/test_config.py</files>
  <action>
Create test_config.py with TestPlexSyncConfig class:

Required field tests:
- test_valid_config_with_required_fields: Use valid_config_dict fixture
- test_plex_url_required: omit plex_url, expect error
- test_plex_token_required: omit plex_token, expect error

URL validation tests:
- test_plex_url_must_be_http: parametrize ["ftp://x", "file://x", "x.com"], expect error
- test_plex_url_http_accepted: "http://localhost:32400" accepted
- test_plex_url_https_accepted: "https://plex.example.com" accepted
- test_plex_url_trailing_slash_removed: "http://x:32400/" becomes "http://x:32400"

Token validation tests:
- test_plex_token_too_short_rejected: token < 10 chars rejected
- test_plex_token_min_length_accepted: 10 char token accepted

Range validation tests (use parametrize):
- test_max_retries_range: [(1, True), (5, True), (20, True), (0, False), (21, False)]
- test_poll_interval_range: [(0.1, True), (60.0, True), (0.05, False), (61.0, False)]
- test_plex_connect_timeout_range: [(1.0, True), (30.0, True), (0.5, False), (31.0, False)]
- test_plex_read_timeout_range: [(5.0, True), (120.0, True), (4.0, False), (121.0, False)]
- test_dlq_retention_days_range: [(1, True), (365, True), (0, False), (366, False)]

Boolean field tests:
- test_strict_matching_accepts_bool: True/False both work
- test_strict_matching_accepts_string: "true"/"false"/"yes"/"no" converted
- test_preserve_plex_edits_accepts_bool: Same pattern
- test_invalid_boolean_rejected: "maybe" raises error

Default value tests:
- test_defaults_applied: Create with only required, verify defaults

validate_config helper tests:
- test_validate_config_success: valid dict returns (config, None)
- test_validate_config_failure: invalid dict returns (None, error_string)
- test_validate_config_multiple_errors: Multiple issues reported in error

Use valid_config_dict fixture from conftest.py.
  </action>
  <verify>pytest tests/validation/test_config.py -v</verify>
  <done>All PlexSyncConfig tests pass with comprehensive range and type validation</done>
</task>

<task type="auto">
  <name>Task 3: Sanitizer Tests</name>
  <files>tests/validation/test_sanitizers.py</files>
  <action>
Create test_sanitizers.py testing sanitize_for_plex:

Basic input handling:
- test_returns_empty_for_none: sanitize_for_plex(None) returns ""
- test_returns_empty_for_empty_string: sanitize_for_plex("") returns ""
- test_preserves_normal_text: "Hello World" unchanged

Control character removal:
- test_removes_null_bytes: "Hello\x00World" becomes "Hello World" (space from split)
- test_removes_control_chars: "\x01\x02test\x1f" cleaned
- test_removes_format_chars: Zero-width chars removed

Smart quote conversion:
- test_converts_left_double_quote: "\u201c" becomes '"'
- test_converts_right_double_quote: "\u201d" becomes '"'
- test_converts_left_single_quote: "\u2018" becomes "'"
- test_converts_right_single_quote: "\u2019" becomes "'"
- test_converts_en_dash: "\u2013" becomes "-"
- test_converts_em_dash: "\u2014" becomes "-"
- test_converts_ellipsis: "\u2026" becomes "..."

Whitespace normalization:
- test_collapses_multiple_spaces: "a  b   c" becomes "a b c"
- test_collapses_tabs: "a\tb" becomes "a b"
- test_collapses_newlines: "a\nb" becomes "a b"
- test_strips_leading_trailing: "  text  " becomes "text"

Truncation:
- test_no_truncation_under_limit: 200 chars with max_length=255 unchanged
- test_truncation_at_max_length: 300 chars with max_length=255 truncated
- test_truncation_prefers_word_boundary: "word word word..." truncated at space
- test_truncation_hard_cut_when_no_good_boundary: Long single word hard truncated
  </action>
  <verify>pytest tests/validation/test_sanitizers.py -v</verify>
  <done>All sanitizer tests pass with edge case coverage</done>
</task>

<task type="auto">
  <name>Task 4: Error Classification Tests and Coverage Verification</name>
  <files>tests/validation/test_errors.py</files>
  <action>
Create test_errors.py testing classify_http_error and classify_exception:

TestClassifyHttpError class:
- test_transient_codes_return_transient_error: parametrize [429, 500, 502, 503, 504], each returns TransientError
- test_permanent_codes_return_permanent_error: parametrize [400, 401, 403, 404, 405, 410, 422], each returns PermanentError
- test_unknown_4xx_returns_permanent: parametrize [418, 451], returns PermanentError
- test_unknown_5xx_returns_transient: parametrize [505, 599], returns TransientError
- test_unexpected_codes_return_transient: parametrize [100, 200, 301], returns TransientError (safety fallback)

TestClassifyException class:
- test_already_transient_returns_transient: TransientError("x") returns TransientError
- test_already_permanent_returns_permanent: PermanentError("x") returns PermanentError
- test_connection_error_is_transient: ConnectionError("conn") returns TransientError
- test_timeout_error_is_transient: TimeoutError("timeout") returns TransientError
- test_os_error_is_transient: OSError("os") returns TransientError
- test_value_error_is_permanent: ValueError("val") returns PermanentError
- test_type_error_is_permanent: TypeError("type") returns PermanentError
- test_key_error_is_permanent: KeyError("key") returns PermanentError
- test_attribute_error_is_permanent: AttributeError("attr") returns PermanentError
- test_unknown_exception_is_transient: Exception("unknown") returns TransientError
- test_http_exception_with_response: Create mock exception with response.status_code, verify classify_http_error called

Mock the logger to avoid debug output noise. Use parametrize for the status code tests.

Run coverage and fix gaps:
pytest tests/validation/ --cov=validation --cov-report=term-missing
Add tests for any uncovered lines to achieve >80% coverage on validation/ module.
  </action>
  <verify>pytest tests/validation/ --cov=validation --cov-report=term-missing --cov-fail-under=80</verify>
  <done>All error classification tests pass, validation module has >80% coverage</done>
</task>

</tasks>

<verification>
1. All tests pass: pytest tests/validation/ -v
2. Coverage threshold met: pytest tests/validation/ --cov=validation --cov-fail-under=80
3. Parametrized tests cover edge cases comprehensively
4. Error messages are tested (not just error occurrence)
5. Error classification covers all branches in errors.py
</verification>

<success_criteria>
- tests/validation/test_metadata.py exists with TestSyncMetadata class
- tests/validation/test_config.py exists with TestPlexSyncConfig class
- tests/validation/test_sanitizers.py exists with sanitizer tests
- tests/validation/test_errors.py exists with TestClassifyHttpError and TestClassifyException classes
- All validation tests pass
- validation module coverage >80%
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-unit-tests/02-02-SUMMARY.md`
</output>
