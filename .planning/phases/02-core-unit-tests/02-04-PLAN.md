---
phase: 02-core-unit-tests
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/hooks/test_handlers.py
autonomous: true

must_haves:
  truths:
    - "on_scene_update filters non-sync events"
    - "on_scene_update skips during active scans"
    - "on_scene_update validates metadata before enqueueing"
    - "requires_plex_sync correctly identifies sync-worthy fields"
    - "is_scan_running detects active Stash jobs"
  artifacts:
    - path: "tests/hooks/test_handlers.py"
      provides: "Hook handler unit tests"
      contains: "class TestOnSceneUpdate"
  key_links:
    - from: "tests/hooks/test_handlers.py"
      to: "hooks/handlers.py"
      via: "imports handler functions"
      pattern: "from hooks.handlers import"
---

<objective>
Create comprehensive unit tests for the hooks module (on_scene_update handler and helper functions).

Purpose: Test hook handler filtering, validation, and enqueueing with mocked dependencies.
Output: Test file covering all handler paths with >80% coverage.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-core-unit-tests/02-RESEARCH.md
@hooks/handlers.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Helper Function Tests</name>
  <files>tests/hooks/test_handlers.py</files>
  <action>
Create test_handlers.py with tests for helper functions:

requires_plex_sync tests:
- test_title_triggers_sync: {"title": "x"} returns True
- test_details_triggers_sync: {"details": "x"} returns True
- test_studio_id_triggers_sync: {"studio_id": 1} returns True
- test_performer_ids_triggers_sync: {"performer_ids": [1]} returns True
- test_tag_ids_triggers_sync: {"tag_ids": [1]} returns True
- test_rating100_triggers_sync: {"rating100": 50} returns True
- test_date_triggers_sync: {"date": "2024-01-01"} returns True
- test_play_count_does_not_trigger: {"play_count": 5} returns False
- test_view_history_does_not_trigger: {"last_played_at": 123} returns False
- test_empty_update_does_not_trigger: {} returns False

is_scan_running tests (mock stash.call_GQL):
- test_no_stash_returns_false: is_scan_running(None) returns False
- test_no_jobs_returns_false: GQL returns empty jobQueue
- test_scan_running_returns_true: GQL returns RUNNING job with "scan" description
- test_generate_running_returns_true: RUNNING job with "generate" description
- test_completed_scan_returns_false: FINISHED job returns False
- test_gql_error_returns_false: Exception during call returns False (safe default)

mark_scene_pending/unmark_scene_pending/is_scene_pending tests:
- test_mark_and_check_pending: Mark scene, verify is_scene_pending returns True
- test_unmark_clears_pending: Mark then unmark, verify False
- test_unmark_nonexistent_safe: Unmark scene never marked (no error)
- test_separate_scenes_independent: Mark scene 1, scene 2 not pending

Import and use:
```python
from hooks.handlers import (
    requires_plex_sync,
    is_scan_running,
    mark_scene_pending,
    unmark_scene_pending,
    is_scene_pending,
    on_scene_update,
)
```

Clear _pending_scene_ids between tests to avoid state pollution:
```python
@pytest.fixture(autouse=True)
def clear_pending_scenes():
    from hooks import handlers
    handlers._pending_scene_ids.clear()
    yield
    handlers._pending_scene_ids.clear()
```
  </action>
  <verify>pytest tests/hooks/test_handlers.py::TestRequiresPlexSync tests/hooks/test_handlers.py::TestIsScanRunning tests/hooks/test_handlers.py::TestPendingScenes -v</verify>
  <done>All helper function tests pass</done>
</task>

<task type="auto">
  <name>Task 2: on_scene_update Tests</name>
  <files>tests/hooks/test_handlers.py</files>
  <action>
Add TestOnSceneUpdate class to test_handlers.py:

Filter tests:
- test_filters_non_sync_events: play_count update returns False
- test_filters_during_scan: Mock is_scan_running=True, returns False
- test_filters_already_pending: Mark scene pending, returns False
- test_filters_already_synced_timestamp: sync_timestamps has newer entry, returns False

File path tests:
- test_returns_false_no_file_path: stash.call_GQL returns scene without files
- test_extracts_file_path_from_scene: Verify path extracted from GQL response

Validation tests (mock validate_metadata):
- test_validates_metadata_before_enqueue: Verify validate_metadata called with correct data
- test_enqueues_on_valid_metadata: Valid metadata -> enqueue called
- test_returns_false_on_title_validation_error: Title error -> returns False
- test_continues_on_non_critical_validation_error: Other errors -> still enqueues with warning

Enqueue tests (mock enqueue):
- test_enqueues_with_correct_structure: Verify enqueue called with scene_id, "metadata", data
- test_marks_scene_pending_after_enqueue: After enqueue, is_scene_pending returns True
- test_returns_true_on_success: Successful enqueue returns True

Timing test:
- test_logs_warning_over_100ms: Mock time to simulate slow execution, verify warning logged

Mock setup for each test:
```python
@pytest.fixture
def mock_stash_gql(mocker):
    stash = MagicMock()
    stash.call_GQL.return_value = {
        "findScene": {
            "id": "123",
            "title": "Test Scene",
            "files": [{"path": "/media/test.mp4"}],
            "studio": {"name": "Studio"},
            "performers": [{"name": "Actor"}],
            "tags": [{"name": "Tag"}],
            "paths": {}
        }
    }
    return stash

@pytest.fixture
def mock_dependencies(mocker):
    mocker.patch('hooks.handlers.is_scan_running', return_value=False)
    mocker.patch('hooks.handlers.enqueue')
    mocker.patch('hooks.handlers.validate_metadata', return_value=(MagicMock(
        title="Test", scene_id=123, details=None, rating100=None,
        date=None, studio=None, performers=None, tags=None
    ), None))
```

Use mock_queue fixture from conftest.py.
  </action>
  <verify>pytest tests/hooks/test_handlers.py::TestOnSceneUpdate -v</verify>
  <done>All on_scene_update tests pass covering filters, validation, and enqueueing</done>
</task>

<task type="auto">
  <name>Task 3: Coverage Verification and Gap Closure</name>
  <files>tests/hooks/test_handlers.py</files>
  <action>
Run coverage report for hooks module:
pytest tests/hooks/ --cov=hooks --cov-report=term-missing

Review uncovered lines and add tests:

Common gaps to check:
- GQL fallback path (_callGraphQL vs call_GQL)
- find_scene fallback when GQL fails
- Edge cases in metadata merging
- Path extraction error handling
- Timing warning branch

Add any missing tests to achieve >80% coverage.

Edge case tests to add if not covered:
- test_gql_fallback_to_find_scene: call_GQL fails, uses find_scene
- test_handles_missing_studio: scene without studio field
- test_handles_missing_performers: scene without performers
- test_handles_missing_tags: scene without tags
- test_handles_missing_paths: scene without paths

Final verification:
pytest tests/hooks/ --cov=hooks --cov-report=term-missing --cov-fail-under=80

Run full test suite to ensure no regressions:
pytest tests/ -v
  </action>
  <verify>pytest tests/hooks/ --cov=hooks --cov-report=term-missing --cov-fail-under=80</verify>
  <done>hooks module has >80% coverage, all tests pass</done>
</task>

</tasks>

<verification>
1. All tests pass: pytest tests/hooks/ -v
2. Coverage threshold met: pytest tests/hooks/ --cov=hooks --cov-fail-under=80
3. No external API calls (all mocked)
4. State pollution prevented with fixture cleanup
</verification>

<success_criteria>
- tests/hooks/test_handlers.py exists with comprehensive handler tests
- Helper functions tested: requires_plex_sync, is_scan_running, pending scene functions
- on_scene_update tested: filters, validation, enqueueing
- All hooks tests pass
- hooks module coverage >80%
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-unit-tests/02-04-SUMMARY.md`
</output>
