---
phase: 22-dlq-recovery-outage-jobs
plan: 02
type: execute
wave: 2
depends_on: ["22-01"]
files_modified:
  - Stash2Plex.py
  - Stash2Plex.yml
  - tests/test_main.py
autonomous: true

must_haves:
  truths:
    - "Recover Outage Jobs task available in Stash UI"
    - "Task identifies DLQ entries from last completed outage window"
    - "Task defaults to PlexServerDown only (conservative)"
    - "Task reports detailed results (recovered, skipped by reason, failed)"
    - "Task is registered in management_modes (no queue drain wait)"
  artifacts:
    - path: "Stash2Plex.py"
      provides: "handle_recover_outage_jobs() task handler"
      contains: "handle_recover_outage_jobs"
    - path: "Stash2Plex.yml"
      provides: "Recover Outage Jobs task registration"
      contains: "recover_outage_jobs"
    - path: "tests/test_main.py"
      provides: "Tests for recovery task handler"
  key_links:
    - from: "Stash2Plex.py"
      to: "sync_queue/dlq_recovery.py"
      via: "import get_outage_dlq_entries, recover_outage_jobs, get_error_types_for_recovery"
      pattern: "from sync_queue.dlq_recovery import"
    - from: "Stash2Plex.py"
      to: "worker/outage_history.py"
      via: "OutageHistory.get_history() for last outage window"
      pattern: "OutageHistory.*get_history"
    - from: "Stash2Plex.py"
      to: "_MANAGEMENT_HANDLERS"
      via: "dispatch table entry for recover_outage_jobs mode"
      pattern: "recover_outage_jobs.*handle_recover_outage_jobs"
---

<objective>
Wire DLQ recovery into Stash UI as "Recover Outage Jobs" task with handler, registration, and tests.

Purpose: Make the DLQ recovery module (from Plan 01) accessible via Stash UI, following established task handler patterns (handle_clear_dlq, handle_outage_summary).

Output: Working end-to-end "Recover Outage Jobs" task in Stash UI.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-dlq-recovery-outage-jobs/22-RESEARCH.md
@.planning/phases/22-dlq-recovery-outage-jobs/22-01-SUMMARY.md

@Stash2Plex.py
@Stash2Plex.yml
@sync_queue/dlq_recovery.py
@worker/outage_history.py
@tests/test_main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add handle_recover_outage_jobs and wire into dispatch</name>
  <files>Stash2Plex.py, Stash2Plex.yml</files>
  <action>
    Add handle_recover_outage_jobs() function to Stash2Plex.py following the exact pattern of
    handle_outage_summary() and handle_clear_dlq(). Place it after handle_outage_summary().

    The handler must:
    1. Get data_dir via get_plugin_data_dir()
    2. Import OutageHistory from worker.outage_history, get format_duration and format_elapsed_since
    3. Import get_outage_dlq_entries, recover_outage_jobs, get_error_types_for_recovery from sync_queue.dlq_recovery
    4. Import DeadLetterQueue from sync_queue.dlq
    5. Load outage history, get completed outages (ended_at is not None)
    6. If no outages: log_info("No outage history found - nothing to recover") and return
    7. If no completed outages: log_info("No completed outages found - recovery requires a resolved outage") and return
    8. Get last completed outage (completed[-1])
    9. Log: "Recovering DLQ jobs from outage: {duration} (ended {elapsed} ago)"
    10. Get error types: get_error_types_for_recovery(include_optional=False) -- always conservative default
       (Note: no args parameter needed for include_optional since we want safe defaults always;
       this keeps it simple and prevents user confusion)
    11. Log: "Recovery filter: {', '.join(error_types)}"
    12. Query DLQ: get_outage_dlq_entries(dlq, last_outage.started_at, last_outage.ended_at, error_types)
    13. If no entries: log_info("No recoverable DLQ entries found for last outage window") and return
    14. Log: "Found {len(entries)} DLQ entries to evaluate"
    15. Call recover_outage_jobs(entries, queue, stash_interface, config.plex_client, data_dir)
       - queue = queue_manager.get_queue() if queue_manager else None
       - If no queue: log_error and return
       - stash = stash_interface global
       - plex_client = Need to create PlexClient instance from config (same pattern as handle_health_check)
    16. Log results via log_info:
       "Recovery complete: {recovered} recovered, {skipped_already_queued} already queued, "
       "{skipped_plex_down} skipped (Plex down), {skipped_scene_missing} skipped (scene missing), "
       "{failed} failed"
    17. If recovered > 0: log_info("Re-queued scene IDs: {recovered_scene_ids}")
    18. Wrap entire body in try/except with log_error + traceback

    Wire into dispatch:
    - Add to _MANAGEMENT_HANDLERS dict: 'recover_outage_jobs': lambda args: handle_recover_outage_jobs()
    - Add 'recover_outage_jobs' to the management_modes set (line ~1452)

    Register in Stash2Plex.yml:
    - Add new task entry after "Outage Summary Report":
      - name: Recover Outage Jobs
      - description: "Re-queue DLQ jobs that failed during last Plex outage (PlexServerDown errors only, requires Plex to be healthy)"
      - defaultArgs: mode: recover_outage_jobs
  </action>
  <verify>
    grep -n "handle_recover_outage_jobs" Stash2Plex.py
    grep -n "recover_outage_jobs" Stash2Plex.py | grep -i "management\|handler"
    grep -n "Recover Outage Jobs" Stash2Plex.yml
  </verify>
  <done>
    handle_recover_outage_jobs() exists in Stash2Plex.py, registered in _MANAGEMENT_HANDLERS
    and management_modes, task registered in Stash2Plex.yml
  </done>
</task>

<task type="auto">
  <name>Task 2: Add tests for recovery task handler</name>
  <files>tests/test_main.py</files>
  <action>
    Add tests to tests/test_main.py following the existing test patterns (e.g., test_handle_outage_summary_*,
    test_handle_queue_status_*). Add a new test class or group tests near the existing outage/DLQ tests.

    Tests to add (6-8 tests):

    1. test_recover_outage_jobs_in_management_handlers:
       Verify 'recover_outage_jobs' key exists in _MANAGEMENT_HANDLERS dict.

    2. test_recover_outage_jobs_in_management_modes:
       Verify 'recover_outage_jobs' is in the management_modes set (line ~1452).

    3. test_handle_recover_outage_jobs_no_outages:
       Mock OutageHistory.get_history() to return [].
       Verify log_info called with "No outage history found".

    4. test_handle_recover_outage_jobs_no_completed_outages:
       Mock OutageHistory.get_history() to return [OutageRecord(started_at=100.0)] (no ended_at).
       Verify log_info called with "No completed outages found".

    5. test_handle_recover_outage_jobs_no_dlq_entries:
       Mock OutageHistory with completed outage, mock get_outage_dlq_entries to return [].
       Verify log_info called with "No recoverable DLQ entries found".

    6. test_handle_recover_outage_jobs_successful_recovery:
       Mock OutageHistory with completed outage, mock get_outage_dlq_entries to return entries,
       mock recover_outage_jobs to return RecoveryResult(total=2, recovered=2, recovered_scene_ids=[1,2]).
       Verify log_info called with "Recovery complete: 2 recovered".

    7. test_handle_recover_outage_jobs_uses_conservative_defaults:
       Mock the chain and verify get_error_types_for_recovery is called with include_optional=False
       (or verify get_outage_dlq_entries receives ["PlexServerDown"] as error_types).

    8. test_recover_outage_jobs_task_registered_in_yml:
       Read Stash2Plex.yml and verify "Recover Outage Jobs" task exists with mode: recover_outage_jobs.

    Use unittest.mock.patch for all external dependencies. Follow the exact mocking patterns
    used by test_handle_outage_summary_* tests.
  </action>
  <verify>
    cd /Users/trekkie/projects/PlexSync && python -m pytest tests/test_main.py -v -k "recover" --tb=short
    cd /Users/trekkie/projects/PlexSync && python -m pytest --tb=short -q
  </verify>
  <done>
    6-8 new tests pass covering handler dispatch, management modes, all handler branches
    (no outages, no entries, successful recovery, conservative defaults).
    Full test suite passes with no regressions (1182+ tests).
  </done>
</task>

</tasks>

<verification>
```bash
# Verify task registration
cd /Users/trekkie/projects/PlexSync && grep "recover_outage_jobs" Stash2Plex.py Stash2Plex.yml

# Run recovery-specific tests
cd /Users/trekkie/projects/PlexSync && python -m pytest tests/test_main.py -v -k "recover" --tb=short

# Run all DLQ recovery tests (plan 01 + plan 02)
cd /Users/trekkie/projects/PlexSync && python -m pytest tests/sync_queue/test_dlq_recovery.py tests/test_main.py -v -k "recover or dlq_recovery" --tb=short

# Full regression suite
cd /Users/trekkie/projects/PlexSync && python -m pytest --tb=short -q

# Coverage check
cd /Users/trekkie/projects/PlexSync && python -m pytest --cov=sync_queue.dlq_recovery --cov=Stash2Plex --cov-report=term-missing --tb=short -q
```
</verification>

<success_criteria>
- "Recover Outage Jobs" task registered in Stash2Plex.yml
- handle_recover_outage_jobs() in Stash2Plex.py with all error/empty paths handled
- Dispatch table and management_modes updated
- 6-8 new tests pass
- Full suite passes (1190+ tests, no regressions)
- Conservative default verified (PlexServerDown only)
</success_criteria>

<output>
After completion, create `.planning/phases/22-dlq-recovery-outage-jobs/22-02-SUMMARY.md`
</output>
