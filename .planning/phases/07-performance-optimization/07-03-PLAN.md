---
phase: 07-performance-optimization
plan: 03
type: execute
wave: 3
depends_on: ["07-02"]
files_modified:
  - worker/processor.py
  - plex/timing.py
  - tests/test_timing.py
autonomous: true

must_haves:
  truths:
    - "Worker processor uses caches when processing jobs"
    - "Timing decorator logs operation duration"
    - "Cache statistics are logged periodically"
  artifacts:
    - path: "worker/processor.py"
      provides: "Cache-integrated job processing"
      contains: "PlexCache"
    - path: "plex/timing.py"
      provides: "Timing decorators for performance measurement"
      min_lines: 30
  key_links:
    - from: "worker/processor.py"
      to: "plex/cache.py"
      via: "cache initialization and usage"
      pattern: "from plex.cache import"
    - from: "worker/processor.py"
      to: "plex/timing.py"
      via: "timing decorator"
      pattern: "from plex.timing import"
---

<objective>
Integrate caching into the worker processor and add timing utilities for performance measurement.

Purpose: Complete the performance optimization by wiring caches into actual job processing and providing timing/logging to verify improvement.

Output: Cache-integrated SyncWorker, timing decorators, and measurable performance logging.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-performance-optimization/07-RESEARCH.md
@.planning/phases/07-performance-optimization/07-01-SUMMARY.md
@.planning/phases/07-performance-optimization/07-02-SUMMARY.md

# Files being modified
@worker/processor.py
@plex/cache.py
@plex/matcher.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create timing utilities module</name>
  <files>plex/timing.py</files>
  <action>
Create plex/timing.py with timing decorators and utilities for performance measurement.

Implementation:
```python
"""
Timing utilities for performance measurement.

Provides decorators to log operation duration and identify bottlenecks.
"""

import time
import logging
from functools import wraps
from typing import Callable, Any

logger = logging.getLogger('PlexSync.plex.timing')

def timed(func: Callable) -> Callable:
    """
    Decorator to log function execution time.

    Logs at DEBUG level for fast operations (<1s),
    INFO level for slower operations (>=1s).

    Example:
        >>> @timed
        ... def slow_function():
        ...     time.sleep(0.5)
        ...     return "done"
        >>> slow_function()  # Logs: slow_function took 0.501s
    """
    @wraps(func)
    def wrapper(*args, **kwargs) -> Any:
        start = time.perf_counter()
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            elapsed = time.perf_counter() - start
            level = logging.INFO if elapsed >= 1.0 else logging.DEBUG
            logger.log(level, f"{func.__name__} took {elapsed:.3f}s")
    return wrapper


class OperationTimer:
    """
    Context manager for timing code blocks.

    Example:
        >>> with OperationTimer("database query"):
        ...     result = db.query()
        # Logs: database query took 0.123s
    """

    def __init__(self, operation_name: str):
        self.operation_name = operation_name
        self.start_time = None
        self.elapsed = None

    def __enter__(self):
        self.start_time = time.perf_counter()
        return self

    def __exit__(self, *args):
        self.elapsed = time.perf_counter() - self.start_time
        level = logging.INFO if self.elapsed >= 1.0 else logging.DEBUG
        logger.log(level, f"{self.operation_name} took {self.elapsed:.3f}s")
```

Also add Stash-compatible log functions for plugin context:
```python
import sys

def log_timing(msg):
    """Log timing message with Stash plugin format."""
    print(f"\x01d\x02[PlexSync Timing] {msg}", file=sys.stderr)
```
  </action>
  <verify>
```python
from plex.timing import timed, OperationTimer
import time

@timed
def test_func():
    time.sleep(0.1)
    return "ok"

result = test_func()
print(result)  # Should print "ok" and log timing
```
  </verify>
  <done>plex/timing.py exists with timed decorator and OperationTimer context manager</done>
</task>

<task type="auto">
  <name>Task 2: Integrate caches into SyncWorker</name>
  <files>worker/processor.py</files>
  <action>
Modify worker/processor.py to use PlexCache and MatchCache for job processing.

Changes required:

1. Add cache initialization in SyncWorker.__init__():
```python
def __init__(self, ...):
    # ... existing init ...

    # Initialize caches (lazy, created on first use)
    self._library_cache: Optional['PlexCache'] = None
    self._match_cache: Optional['MatchCache'] = None
```

2. Add lazy cache initialization method:
```python
def _get_caches(self) -> tuple['PlexCache', 'MatchCache']:
    """Get or create cache instances."""
    if self._library_cache is None:
        from plex.cache import PlexCache, MatchCache
        cache_dir = os.path.join(self.data_dir, 'cache') if self.data_dir else None
        if cache_dir:
            self._library_cache = PlexCache(cache_dir)
            self._match_cache = MatchCache(cache_dir)
    return self._library_cache, self._match_cache
```

3. In _process_job(), pass caches to find_plex_items_with_confidence():
```python
# Get caches
library_cache, match_cache = self._get_caches()

# Pass to matcher
confidence, item, candidates = find_plex_items_with_confidence(
    section,
    file_path,
    library_cache=library_cache,
    match_cache=match_cache,
)
```

4. Add periodic cache stats logging (every 10 jobs, same pattern as DLQ logging):
```python
# In job processing success path
if self._jobs_since_dlq_log >= self._dlq_log_interval:
    self._log_dlq_status()
    self._log_cache_stats()  # NEW
    self._jobs_since_dlq_log = 0
```

5. Add _log_cache_stats() method:
```python
def _log_cache_stats(self):
    """Log cache hit/miss statistics."""
    library_cache, match_cache = self._get_caches()
    if library_cache:
        stats = library_cache.get_stats()
        if stats.get('hits', 0) + stats.get('misses', 0) > 0:
            hit_rate = stats['hits'] / (stats['hits'] + stats['misses']) * 100
            log_debug(f"Library cache: {hit_rate:.1f}% hit rate ({stats['hits']} hits, {stats['misses']} misses)")
    if match_cache:
        stats = match_cache.get_stats()
        if stats.get('hits', 0) + stats.get('misses', 0) > 0:
            hit_rate = stats['hits'] / (stats['hits'] + stats['misses']) * 100
            log_info(f"Match cache: {hit_rate:.1f}% hit rate ({stats['hits']} hits, {stats['misses']} misses)")
```

6. Import timing decorator and apply to _process_job:
```python
from plex.timing import timed

# Apply to _process_job
@timed
def _process_job(self, job: dict):
    ...
```

Handle the case where data_dir is None (no caching) gracefully - caches are optional.
  </action>
  <verify>
Worker should still work without data_dir (no caching):
```python
from worker.processor import SyncWorker
# Should not crash when instantiated with data_dir=None
```
  </verify>
  <done>SyncWorker uses caches when data_dir is set, logs cache statistics</done>
</task>

<task type="auto">
  <name>Task 3: Add tests for timing and processor integration</name>
  <files>tests/test_timing.py, tests/integration/test_processor.py</files>
  <action>
Create tests/test_timing.py with tests for timing utilities:

1. timed decorator logs execution time
2. OperationTimer context manager works
3. Slow operations logged at INFO level
4. Fast operations logged at DEBUG level

Update tests/integration/test_processor.py (if exists) or create new tests:

1. SyncWorker initializes caches when data_dir provided
2. SyncWorker works without caches when data_dir is None
3. Cache stats logging doesn't crash with empty caches

Use caplog fixture to verify logging output.
  </action>
  <verify>Run `pytest tests/test_timing.py -v` and all tests pass</verify>
  <done>Timing and processor integration tests pass</done>
</task>

</tasks>

<verification>
1. `python -c "from plex.timing import timed; print('OK')"` succeeds
2. `pytest tests/test_timing.py -v` all tests pass
3. `pytest tests/ -v --ignore=tests/integration` all unit tests pass
4. Worker processes jobs and logs timing/cache stats
</verification>

<success_criteria>
- Timing utilities provide performance measurement
- SyncWorker uses caches for job processing when data_dir is set
- Cache statistics logged periodically
- All existing tests still pass
- Phase goal achieved: measurable reduction in Plex API calls through caching
</success_criteria>

<output>
After completion, create `.planning/phases/07-performance-optimization/07-03-SUMMARY.md`
</output>
