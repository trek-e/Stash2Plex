---
phase: 20-graduated-recovery-rate-limiting
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - worker/rate_limiter.py
  - tests/worker/test_rate_limiter.py
autonomous: true

must_haves:
  truths:
    - "RecoveryRateLimiter calculates graduated rate from initial_rate to target_rate over ramp_duration"
    - "Token bucket controls burst: jobs wait when no tokens available"
    - "Error rate monitoring triggers backoff when failure rate exceeds threshold"
    - "Recovery period ends cleanly after ramp_duration, returning to unlimited rate"
    - "Rate limiter state is reconstructable from recovery_started_at timestamp"
  artifacts:
    - path: "worker/rate_limiter.py"
      provides: "RecoveryRateLimiter class with token bucket, graduated scaling, and error monitoring"
      exports: ["RecoveryRateLimiter"]
    - path: "tests/worker/test_rate_limiter.py"
      provides: "Comprehensive tests for rate limiter"
      min_lines: 200
  key_links:
    - from: "worker/rate_limiter.py"
      to: "time.time()"
      via: "time-based rate calculation"
      pattern: "time\\.time\\(\\)"
    - from: "worker/rate_limiter.py"
      to: "recovery_started_at"
      via: "elapsed time since recovery determines current rate"
      pattern: "recovery_started_at"
---

<objective>
TDD implementation of RecoveryRateLimiter class that provides graduated rate limiting during post-recovery queue drain.

Purpose: After Plex recovers from outage, the worker loop must drain the backlog at a graduated rate (5 -> 20 jobs/sec over 5 minutes) to avoid overwhelming the just-recovered server. Error rate monitoring backs off if failures spike.

Output: `worker/rate_limiter.py` with full test suite
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-graduated-recovery-rate-limiting/20-RESEARCH.md
@worker/circuit_breaker.py
@worker/recovery.py
@worker/backoff.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD — RecoveryRateLimiter with graduated scaling, token bucket, and error monitoring</name>
  <files>worker/rate_limiter.py, tests/worker/test_rate_limiter.py</files>
  <action>
**RED phase — Write failing tests first in `tests/worker/test_rate_limiter.py`:**

Test categories to cover (target ~30-40 tests):

1. **Graduated rate calculation (6-8 tests):**
   - `current_rate()` returns `initial_rate` (5.0) at elapsed=0
   - `current_rate()` returns midpoint rate at elapsed=ramp_duration/2
   - `current_rate()` returns `target_rate` (20.0) at elapsed=ramp_duration
   - `current_rate()` returns `target_rate` when elapsed > ramp_duration
   - Linear interpolation: rate = initial + (target - initial) * (elapsed / ramp_duration)
   - Custom config: initial_rate=2, target_rate=50, ramp_duration=600

2. **Recovery period lifecycle (5-6 tests):**
   - `is_in_recovery_period()` returns False when not started (recovery_started_at=0)
   - `is_in_recovery_period()` returns True during recovery
   - `is_in_recovery_period()` returns False after ramp_duration elapsed
   - `start_recovery_period()` sets recovery_started_at and resets tokens
   - `end_recovery_period()` clears recovery_started_at (sets to 0.0)
   - `start_recovery_period()` from existing timestamp (cross-restart resume)

3. **Token bucket / should_wait (6-8 tests):**
   - `should_wait()` returns 0.0 when not in recovery period (no limiting)
   - `should_wait()` returns 0.0 when token available (consumes token)
   - `should_wait()` returns >0 wait time when no tokens available
   - Token refill: after waiting, tokens refill based on current_rate
   - Burst capacity: bucket starts with `capacity` tokens (default 1.0)
   - Rate changes: as current_rate increases, refill accelerates
   - At recovery end: should_wait() returns 0.0 (unlimited)

4. **Error rate monitoring (6-8 tests):**
   - `record_result(success=True)` records success
   - `record_result(success=False)` records failure
   - `error_rate()` calculates failures/total in time window (60s)
   - Old results outside window are pruned
   - `should_backoff()` returns True when error_rate > threshold (0.3)
   - `should_backoff()` returns False when error_rate <= threshold
   - Backoff reduces current rate by 50% (rate_multiplier=0.5)
   - Backoff recovery: when error rate drops below 0.1, restore multiplier to 1.0

5. **Edge cases (4-5 tests):**
   - Constructor defaults: initial_rate=5.0, target_rate=20.0, ramp_duration=300.0, error_threshold=0.3
   - `now` parameter injection for deterministic testing (all time-dependent methods accept optional `now`)
   - Empty error window returns 0.0 error rate
   - Multiple start_recovery_period calls reset properly
   - Thread safety not needed (single worker thread per plugin invocation)

All tests should use `now` parameter injection for deterministic testing (no real time.time() calls in tests).

Run tests: `pytest tests/worker/test_rate_limiter.py -v` — ALL MUST FAIL (class doesn't exist).

Commit: `test(20-01): add failing tests for RecoveryRateLimiter`

**GREEN phase — Implement `worker/rate_limiter.py`:**

```python
"""
Recovery rate limiter for graduated queue drain after Plex outage recovery.

Uses token bucket algorithm with graduated rate scaling to prevent
overwhelming a just-recovered Plex server. Error rate monitoring
triggers adaptive backoff if failures spike during recovery.
"""

import time
from typing import Optional

from shared.log import create_logger
_, log_debug, log_info, log_warn, _ = create_logger("RateLimiter")
```

Class `RecoveryRateLimiter`:

**Constructor params:**
- `initial_rate: float = 5.0` — Starting rate (jobs/sec)
- `target_rate: float = 20.0` — Full rate (jobs/sec)
- `ramp_duration: float = 300.0` — Seconds to reach full rate (5 min)
- `error_threshold: float = 0.3` — Error rate triggering backoff (30%)
- `error_window: float = 60.0` — Time window for error rate calculation (seconds)

**Internal state:**
- `recovery_started_at: float = 0.0` — 0.0 means not in recovery
- `tokens: float = 1.0` — Current token bucket level
- `capacity: float = 1.0` — Max tokens (allows 1 job burst)
- `last_update: float = 0.0` — Last token refill time
- `rate_multiplier: float = 1.0` — Backoff multiplier (0.5 during backoff, 1.0 normal)
- `backoff_until: float = 0.0` — Time when backoff expires
- `results: list` — List of (timestamp, success_bool) for error window

**Methods:**

1. `is_in_recovery_period(now=None) -> bool`: Check if recovery_started_at > 0 and elapsed < ramp_duration.

2. `start_recovery_period(now=None)`: Set recovery_started_at, reset tokens/last_update, clear error results and backoff state. Log at info level.

3. `end_recovery_period()`: Set recovery_started_at=0.0, clear state. Log at info level.

4. `current_rate(now=None) -> float`: Linear interpolation from initial_rate to target_rate based on elapsed time. Apply rate_multiplier. If not in recovery, return target_rate.

5. `should_wait(now=None) -> float`: Return 0.0 if not in recovery period. Otherwise: refill tokens based on elapsed * current_rate(), try to consume 1 token. If available, return 0.0. If not, return shortage/current_rate() (seconds to wait).

6. `record_result(success: bool, now=None)`: Append to results list with timestamp. Prune results outside error_window. Call `_maybe_adjust_rate(now)`.

7. `_maybe_adjust_rate(now=None)`: If not in recovery, return. Check error_rate(). If > error_threshold and not already backed off: set rate_multiplier=0.5, backoff_until=now+60, log warning. If error_rate < 0.1 and now > backoff_until: restore rate_multiplier=1.0, log info.

8. `error_rate(now=None) -> float`: Calculate failures/total in results within error_window. Return 0.0 if no results.

Run tests: `pytest tests/worker/test_rate_limiter.py -v` — ALL MUST PASS.

Commit: `feat(20-01): implement RecoveryRateLimiter with graduated scaling and error monitoring`

**REFACTOR phase (if needed):** Clean up any duplication, ensure docstrings complete.
  </action>
  <verify>
`pytest tests/worker/test_rate_limiter.py -v` — all tests pass.
`pytest --tb=short` — full suite passes (no regressions, should be 1086+ tests).
`python -c "from worker.rate_limiter import RecoveryRateLimiter; print('import OK')"` — clean import.
  </verify>
  <done>
RecoveryRateLimiter class exists with:
- Graduated rate calculation (linear ramp 5->20 over 5min)
- Token bucket controlling job throughput during recovery
- Error rate monitoring with adaptive backoff (30% threshold)
- Clean recovery period lifecycle (start/end/is_in)
- All time-dependent methods accept `now` parameter for testing
- 30+ tests covering all behaviors
  </done>
</task>

</tasks>

<verification>
- `pytest tests/worker/test_rate_limiter.py -v` — all tests pass
- `pytest --tb=short` — full suite passes, no regressions
- `python -c "from worker.rate_limiter import RecoveryRateLimiter; r = RecoveryRateLimiter(); print(r.current_rate())"` — prints 20.0 (target rate, not in recovery)
</verification>

<success_criteria>
- RecoveryRateLimiter.current_rate() returns graduated rate during recovery period
- RecoveryRateLimiter.should_wait() returns wait time based on token bucket during recovery, 0.0 outside
- RecoveryRateLimiter.record_result() tracks error rate and triggers adaptive backoff
- All tests pass, full suite has no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/20-graduated-recovery-rate-limiting/20-01-SUMMARY.md`
</output>
