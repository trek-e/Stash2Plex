---
phase: 01-persistent-queue-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - queue/dlq.py
  - queue/__init__.py
autonomous: true

must_haves:
  truths:
    - "Failed jobs can be moved to dead letter queue"
    - "DLQ entries persist across process restart"
    - "DLQ entries can be queried for manual review"
    - "DLQ stores job data, error details, and failure timestamp"
  artifacts:
    - path: "queue/dlq.py"
      provides: "Dead letter queue implementation"
      contains: "class DeadLetterQueue"
      exports: ["DeadLetterQueue"]
    - path: "queue/__init__.py"
      provides: "Updated public API"
      contains: "DeadLetterQueue"
  key_links:
    - from: "queue/dlq.py"
      to: "sqlite3"
      via: "uses SQLite for storage"
      pattern: "sqlite3.connect"
---

<objective>
Implement the Dead Letter Queue for permanently failed jobs.

Purpose: When jobs fail beyond retry limits or encounter permanent errors, they need a separate storage for manual review. The DLQ prevents failed jobs from blocking the main queue while preserving them for debugging.

Output: DeadLetterQueue class that stores failed jobs with error context in a separate SQLite table.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/01-persistent-queue-foundation/01-CONTEXT.md
@.planning/phases/01-persistent-queue-foundation/01-RESEARCH.md
@.planning/phases/01-persistent-queue-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement DeadLetterQueue class</name>
  <files>
    queue/dlq.py
  </files>
  <action>
Create queue/dlq.py with DeadLetterQueue class following RESEARCH.md Pattern 2:

**__init__(self, data_dir: str):**
- Store db_path as data_dir/dlq.db
- Call _setup_schema()

**_setup_schema(self):**
- Create dead_letters table if not exists:
  - id: INTEGER PRIMARY KEY AUTOINCREMENT
  - job_id: INTEGER (original pqid from main queue)
  - scene_id: INTEGER (for easy querying)
  - job_data: BLOB (pickled full job dict)
  - error_type: TEXT (exception class name)
  - error_message: TEXT (str(exception))
  - stack_trace: TEXT (full traceback)
  - retry_count: INTEGER (how many retries before failure)
  - failed_at: TIMESTAMP DEFAULT CURRENT_TIMESTAMP
- Create index on failed_at for recent query efficiency
- Create index on scene_id for per-scene queries

**add(self, job: dict, error: Exception, retry_count: int):**
- Connect to db
- Insert job with pickled job_data, error details, traceback.format_exc()
- Commit and close
- Log warning: "Job {pqid} moved to DLQ after {retry_count} retries: {error_type}"

**get_recent(self, limit: int = 10) -> list[dict]:**
- Query recent entries ordered by failed_at DESC
- Return list of dicts with: id, job_id, scene_id, error_type, error_message, failed_at
- Don't unpickle job_data by default (for listing)

**get_by_id(self, dlq_id: int) -> Optional[dict]:**
- Query single entry by id
- Unpickle and return full job_data
- Return None if not found

**get_count(self) -> int:**
- Return total count of DLQ entries

**delete_older_than(self, days: int):**
- Delete entries older than N days
- Log: "Removed {count} DLQ entries older than {days} days"
- Per CONTEXT.md, retention period is Claude's discretion - default to 30 days

Use context manager pattern for SQLite connections:
```python
def _get_connection(self):
    return sqlite3.connect(self.db_path)
```

Import pickle and traceback at module level.
  </action>
  <verify>
    - Python syntax check: `python -m py_compile queue/dlq.py`
    - Import works: `python -c "from queue.dlq import DeadLetterQueue"`
  </verify>
  <done>
    - DeadLetterQueue class implemented with add, get_recent, get_by_id, get_count, delete_older_than
    - Schema creates dead_letters table with appropriate columns and indexes
    - Jobs are pickled for storage, unpickled on retrieval
  </done>
</task>

<task type="auto">
  <name>Task 2: Update queue module exports and add integration test</name>
  <files>
    queue/__init__.py
  </files>
  <action>
Update queue/__init__.py to export DeadLetterQueue:
- Add: from queue.dlq import DeadLetterQueue
- Update __all__ if used

Create a simple integration verification (inline or as doctest):

```python
# In queue/dlq.py, add at end:
if __name__ == "__main__":
    import tempfile
    import os

    with tempfile.TemporaryDirectory() as tmpdir:
        dlq = DeadLetterQueue(tmpdir)

        # Add a failed job
        test_job = {"pqid": 1, "scene_id": 123, "data": {"title": "Test"}}
        test_error = ValueError("Plex API error: 404 Not Found")
        dlq.add(test_job, test_error, retry_count=5)

        # Query
        assert dlq.get_count() == 1
        recent = dlq.get_recent(limit=5)
        assert len(recent) == 1
        assert recent[0]["scene_id"] == 123

        # Get full job
        full = dlq.get_by_id(recent[0]["id"])
        assert full["scene_id"] == 123

        print("DLQ integration test PASSED")
```
  </action>
  <verify>
    - Import from package works: `python -c "from queue import DeadLetterQueue"`
    - Self-test passes: `python queue/dlq.py`
  </verify>
  <done>
    - DeadLetterQueue exported from queue package
    - Integration test passes showing add, query, and retrieval work
  </done>
</task>

</tasks>

<verification>
```bash
# Verify DLQ works with main queue
python -c "
from queue.dlq import DeadLetterQueue
import tempfile

with tempfile.TemporaryDirectory() as tmpdir:
    dlq = DeadLetterQueue(tmpdir)
    job = {'pqid': 42, 'scene_id': 999, 'data': {}}
    dlq.add(job, Exception('test failure'), retry_count=3)
    assert dlq.get_count() == 1
    print('DLQ verification PASSED')
"
```
</verification>

<success_criteria>
- [ ] queue/dlq.py implements DeadLetterQueue with all required methods
- [ ] dead_letters table schema includes job_data, error details, timestamps
- [ ] Indexes exist on failed_at and scene_id
- [ ] Jobs are pickled/unpickled correctly
- [ ] DeadLetterQueue is exported from queue package
- [ ] Self-test in dlq.py passes
</success_criteria>

<output>
After completion, create `.planning/phases/01-persistent-queue-foundation/01-02-SUMMARY.md`
</output>
