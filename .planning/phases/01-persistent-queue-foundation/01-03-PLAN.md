---
phase: 01-persistent-queue-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - hooks/__init__.py
  - hooks/handlers.py
  - worker/__init__.py
  - worker/processor.py
  - PlexSync.py
autonomous: true

must_haves:
  truths:
    - "Hook handler completes in <100ms"
    - "Hook handler filters non-sync events before enqueueing"
    - "Background worker processes jobs from queue"
    - "Worker acknowledges successful jobs, nacks transient failures"
    - "Worker moves permanently failed jobs to DLQ"
    - "Plugin entry point initializes queue and starts worker"
  artifacts:
    - path: "hooks/handlers.py"
      provides: "Stash event handlers"
      contains: "def on_scene_update"
    - path: "worker/processor.py"
      provides: "Background job processor"
      contains: "class SyncWorker"
    - path: "PlexSync.py"
      provides: "Plugin entry point"
      contains: "if __name__"
  key_links:
    - from: "hooks/handlers.py"
      to: "queue/operations.py"
      via: "enqueue call"
      pattern: "enqueue\\("
    - from: "worker/processor.py"
      to: "queue/operations.py"
      via: "get_pending, ack_job, nack_job, fail_job"
      pattern: "(get_pending|ack_job|nack_job|fail_job)"
    - from: "worker/processor.py"
      to: "queue/dlq.py"
      via: "move to DLQ on permanent failure"
      pattern: "dlq\\.add"
    - from: "PlexSync.py"
      to: "queue/manager.py"
      via: "initializes QueueManager"
      pattern: "QueueManager"
---

<objective>
Create the hook handler, background worker, and plugin entry point.

Purpose: Connect the queue infrastructure to Stash's event system and Plex sync. The hook handler captures metadata changes quickly (<100ms), the worker processes them reliably with proper acknowledgment, and the entry point ties everything together.

Output: Complete plugin structure ready for Phase 2 (validation) and Phase 3 (Plex API client).
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/01-persistent-queue-foundation/01-CONTEXT.md
@.planning/phases/01-persistent-queue-foundation/01-RESEARCH.md
@.planning/phases/01-persistent-queue-foundation/01-01-SUMMARY.md
@.planning/phases/01-persistent-queue-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement hook handler for fast event capture</name>
  <files>
    hooks/__init__.py
    hooks/handlers.py
  </files>
  <action>
**hooks/__init__.py:**
- Export: from hooks.handlers import on_scene_update, requires_plex_sync

**hooks/handlers.py:**
Create fast, non-blocking event handlers per RESEARCH.md hook handler pattern:

**requires_plex_sync(update_data: dict) -> bool:**
- Return True if update contains sync-worthy changes
- Per CONTEXT.md: "Create jobs only for relevant metadata-changing updates"
- Check for presence of metadata fields: title, details, studio, performers, tags, rating
- Return False for: play count updates, view history, file system only changes
- Simple heuristic: if update_data has any of ["title", "details", "studio_id", "performer_ids", "tag_ids", "rating100"], return True

**on_scene_update(scene_id: int, update_data: dict, queue) -> bool:**
- Record start time
- Check requires_plex_sync(update_data) - if False, log debug and return False
- Create job via enqueue(queue, scene_id, "metadata", update_data)
- Calculate elapsed_ms = (time.time() - start) * 1000
- Log: "Enqueued sync job for scene {scene_id} in {elapsed_ms:.1f}ms"
- If elapsed_ms > 100: log warning "Hook handler exceeded 100ms target"
- Return True

Keep handler minimal - no Plex API calls, no validation, just enqueue and return.

Logging: Use print() for now (stashapi.log integration in later task or phase).
The <100ms requirement means: SQLite insert only, no network, no heavy processing.
  </action>
  <verify>
    - Python syntax check: `python -m py_compile hooks/handlers.py`
    - Import works: `python -c "from hooks.handlers import on_scene_update, requires_plex_sync"`
  </verify>
  <done>
    - requires_plex_sync filters non-sync events
    - on_scene_update completes quickly with timing logged
    - No Plex API calls or heavy processing in handler
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement background worker with acknowledgment workflow</name>
  <files>
    worker/__init__.py
    worker/processor.py
  </files>
  <action>
**worker/__init__.py:**
- Export: from worker.processor import SyncWorker

**worker/processor.py:**
Create SyncWorker class following RESEARCH.md background worker pattern:

**__init__(self, queue, dlq: DeadLetterQueue, max_retries: int = 5):**
- Store queue, dlq, max_retries
- self.running = False
- self.thread = None
- self._retry_counts: dict[int, int] = {}  # pqid -> retry count

**start(self):**
- Set running = True
- Create daemon thread targeting _worker_loop
- Start thread
- Log: "Sync worker started"

**stop(self):**
- Set running = False
- Join thread with timeout=5
- Log: "Sync worker stopped"

**_worker_loop(self):**
```python
while self.running:
    try:
        item = get_pending(self.queue, timeout=10)
        if item is None:
            continue

        pqid = item.get('pqid')
        log(f"Processing job {pqid} for scene {item['scene_id']}")

        try:
            self._process_job(item)
            ack_job(self.queue, item)
            self._retry_counts.pop(pqid, None)
            log(f"Job {pqid} completed")

        except TransientError as e:
            # Transient = retry
            retry_count = self._retry_counts.get(pqid, 0) + 1
            self._retry_counts[pqid] = retry_count

            if retry_count >= self.max_retries:
                log(f"Job {pqid} exceeded max retries, moving to DLQ")
                fail_job(self.queue, item)
                self.dlq.add(item, e, retry_count)
                self._retry_counts.pop(pqid, None)
            else:
                log(f"Job {pqid} failed (attempt {retry_count}), will retry: {e}")
                nack_job(self.queue, item)

        except PermanentError as e:
            # Permanent = DLQ immediately
            log(f"Job {pqid} permanent failure, moving to DLQ: {e}")
            fail_job(self.queue, item)
            self.dlq.add(item, e, self._retry_counts.get(pqid, 0))
            self._retry_counts.pop(pqid, None)

        except Exception as e:
            # Unknown error = treat as transient
            log(f"Job {pqid} unexpected error: {e}")
            nack_job(self.queue, item)

    except Exception as e:
        log(f"Worker loop error: {e}")
        time.sleep(1)  # Avoid tight loop on persistent errors
```

**_process_job(self, job: dict):**
- STUB for now: just log "Would sync scene {scene_id} to Plex"
- This will be implemented in Phase 3 (Plex API Client)
- For now, just pass (success)

**Define exception types at module level:**
```python
class TransientError(Exception):
    """Retry-able errors (network, timeout, 5xx)"""
    pass

class PermanentError(Exception):
    """Non-retry-able errors (4xx except 429, validation)"""
    pass
```

Import from queue.operations: get_pending, ack_job, nack_job, fail_job
Import time, threading
  </action>
  <verify>
    - Python syntax check: `python -m py_compile worker/processor.py`
    - Import works: `python -c "from worker.processor import SyncWorker, TransientError, PermanentError"`
  </verify>
  <done>
    - SyncWorker runs in daemon thread
    - Acknowledges successful jobs
    - Nacks transient failures for retry (up to max_retries)
    - Moves to DLQ on permanent errors or max retries exceeded
    - _process_job is stubbed for Phase 3
  </done>
</task>

<task type="auto">
  <name>Task 3: Create plugin entry point</name>
  <files>
    PlexSync.py
  </files>
  <action>
Create PlexSync.py as the plugin entry point:

```python
#!/usr/bin/env python3
"""
PlexSync - Stash plugin for syncing metadata to Plex
"""
import os
import sys
import json

# Add plugin directory to path for imports
PLUGIN_DIR = os.path.dirname(os.path.abspath(__file__))
if PLUGIN_DIR not in sys.path:
    sys.path.insert(0, PLUGIN_DIR)

from queue.manager import QueueManager
from queue.dlq import DeadLetterQueue
from worker.processor import SyncWorker
from hooks.handlers import on_scene_update

# Globals (initialized in main)
queue_manager = None
dlq = None
worker = None


def get_plugin_data_dir():
    """Get or create plugin data directory."""
    # Check for Stash-provided path first
    data_dir = os.getenv('STASH_PLUGIN_DATA')
    if not data_dir:
        # Default to plugin_dir/data
        data_dir = os.path.join(PLUGIN_DIR, 'data')
    os.makedirs(data_dir, exist_ok=True)
    return data_dir


def initialize():
    """Initialize queue, DLQ, and worker."""
    global queue_manager, dlq, worker

    data_dir = get_plugin_data_dir()
    print(f"[PlexSync] Initializing with data directory: {data_dir}")

    # Initialize queue infrastructure
    queue_manager = QueueManager(data_dir)
    dlq = DeadLetterQueue(data_dir)

    # Start background worker
    worker = SyncWorker(queue_manager.get_queue(), dlq)
    worker.start()

    print("[PlexSync] Initialization complete")


def shutdown():
    """Clean shutdown of worker and queue."""
    global worker, queue_manager

    if worker:
        print("[PlexSync] Stopping worker...")
        worker.stop()

    if queue_manager:
        print("[PlexSync] Shutting down queue...")
        queue_manager.shutdown()

    print("[PlexSync] Shutdown complete")


def handle_hook(hook_context: dict):
    """
    Handle incoming Stash hook.

    Expected hook_context structure:
    {
        "type": "Scene.Update.Post",
        "input": {...scene update data...}
    }
    """
    hook_type = hook_context.get("type", "")
    input_data = hook_context.get("input", {})

    if hook_type == "Scene.Update.Post":
        scene_id = input_data.get("id")
        if scene_id:
            on_scene_update(scene_id, input_data, queue_manager.get_queue())


def main():
    """Main entry point for Stash plugin."""
    # Read input from stdin (Stash plugin protocol)
    input_data = json.loads(sys.stdin.read())

    # Initialize on first call
    global queue_manager
    if queue_manager is None:
        initialize()

    # Handle the hook
    if "args" in input_data and "hookContext" in input_data["args"]:
        handle_hook(input_data["args"]["hookContext"])

    # Return empty response (success)
    print(json.dumps({"output": "ok"}))


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(json.dumps({"error": str(e)}), file=sys.stderr)
        sys.exit(1)
```

Note: This is a basic Stash plugin structure. The actual Stash hook protocol may need adjustment based on stashapi patterns. For now, this provides the integration points.

The key structure is:
1. initialize() - sets up queue infrastructure
2. handle_hook() - dispatches to handlers based on hook type
3. main() - reads JSON from stdin, processes, returns JSON response
  </action>
  <verify>
    - Python syntax check: `python -m py_compile PlexSync.py`
    - Can run without error: `echo '{"args":{}}' | python PlexSync.py` (should output {"output": "ok"})
  </verify>
  <done>
    - PlexSync.py is executable entry point
    - Initializes QueueManager, DeadLetterQueue, SyncWorker
    - Handles Scene.Update.Post hooks
    - Clean shutdown path exists
  </done>
</task>

</tasks>

<verification>
Full integration test:
```bash
# Create test input
echo '{"args":{"hookContext":{"type":"Scene.Update.Post","input":{"id":123,"title":"Test Scene"}}}}' > /tmp/test_hook.json

# Run plugin (will initialize and process hook)
cd /Users/trekkie/projects/PlexSync
cat /tmp/test_hook.json | python PlexSync.py

# Should output: {"output": "ok"}
# Worker runs in background, processes the job
```

Verify queue state:
```python
import os
from queue.manager import QueueManager
from queue.operations import get_stats

data_dir = os.path.join(os.path.dirname(__file__), 'data')
stats = get_stats(os.path.join(data_dir, 'queue'))
print(f"Queue stats: {stats}")
```
</verification>

<success_criteria>
- [ ] hooks/handlers.py implements on_scene_update with <100ms target
- [ ] requires_plex_sync filters non-metadata updates
- [ ] worker/processor.py implements SyncWorker with proper ack/nack/fail workflow
- [ ] TransientError and PermanentError exceptions defined
- [ ] PlexSync.py initializes all components
- [ ] Plugin handles Scene.Update.Post hooks
- [ ] All files pass Python syntax check
</success_criteria>

<output>
After completion, create `.planning/phases/01-persistent-queue-foundation/01-03-SUMMARY.md`
</output>
