---
phase: 01-persistent-queue-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - queue/__init__.py
  - queue/manager.py
  - queue/models.py
  - queue/operations.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Queue manager initializes SQLite database with WAL mode"
    - "Jobs can be enqueued and persist across Python process restart"
    - "Jobs can be queried by status (pending, in_progress, completed, failed)"
    - "Queue operations are thread-safe"
  artifacts:
    - path: "queue/manager.py"
      provides: "Queue initialization and lifecycle"
      contains: "SQLiteAckQueue"
    - path: "queue/models.py"
      provides: "SyncJob data structure"
      contains: "class SyncJob"
    - path: "queue/operations.py"
      provides: "Queue CRUD operations"
      exports: ["enqueue", "get_pending", "update_status", "get_stats"]
    - path: "requirements.txt"
      provides: "Project dependencies"
      contains: "persist-queue"
  key_links:
    - from: "queue/operations.py"
      to: "queue/manager.py"
      via: "imports queue instance"
      pattern: "from.*manager.*import"
    - from: "queue/operations.py"
      to: "queue/models.py"
      via: "uses SyncJob model"
      pattern: "SyncJob"
---

<objective>
Create the persistent queue infrastructure using persist-queue's SQLiteAckQueue.

Purpose: This is the foundation for reliable sync - jobs must survive crashes, restarts, and Plex outages. Without durable persistence, sync failures are silent and unrecoverable.

Output: Working queue module with manager (init/shutdown), models (job structure), and operations (enqueue/dequeue/status).
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-persistent-queue-foundation/01-CONTEXT.md
@.planning/phases/01-persistent-queue-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project structure and dependencies</name>
  <files>
    requirements.txt
    queue/__init__.py
  </files>
  <action>
Create requirements.txt with:
- persist-queue>=1.1.0
- stashapi (for later phases, include now)

Create queue/__init__.py that exports the public API:
- from queue.manager import QueueManager
- from queue.models import SyncJob
- from queue.operations import enqueue, get_pending, update_status, get_stats

Note: The queue/ directory structure follows the pattern from RESEARCH.md.
  </action>
  <verify>
    - File exists: requirements.txt with persist-queue>=1.1.0
    - File exists: queue/__init__.py with imports
    - Python can import: `python -c "import queue"` (no syntax errors)
  </verify>
  <done>
    - requirements.txt lists persist-queue>=1.1.0 and stashapi
    - queue/__init__.py exists with public API exports
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement queue manager and job model</name>
  <files>
    queue/manager.py
    queue/models.py
  </files>
  <action>
**queue/models.py:**
Create SyncJob dataclass (or TypedDict for simpler serialization):
- scene_id: int (Stash scene ID)
- update_type: str ("metadata", "image", etc.)
- data: dict (the metadata to sync)
- enqueued_at: float (timestamp)
- job_key: str (for deduplication, format: "scene_{scene_id}")

Use simple dict-compatible structure (persist-queue pickles data, JSON-serializable dicts are safest per RESEARCH.md pitfall #5).

**queue/manager.py:**
Create QueueManager class:
- __init__(self, data_dir: str): Initialize queue path, create directory if needed
- _init_queue(): Create SQLiteAckQueue with:
  - auto_commit=True (required for AckQueue)
  - multithreading=True (thread-safe)
  - auto_resume=True (recover unack jobs on crash)
- get_queue(): Return the SQLiteAckQueue instance
- shutdown(): Clean shutdown (log stats, close connections if needed)

On init:
1. Create data directory: os.makedirs(data_dir, exist_ok=True)
2. Initialize queue at data_dir/queue/
3. Log "Queue initialized at {path}" via stashapi.log or print (stashapi optional for now)

Per CONTEXT.md: Store in Stash plugin data directory. Use os.getenv('STASH_PLUGIN_DATA', default_path) pattern.
  </action>
  <verify>
    - Python syntax check: `python -m py_compile queue/manager.py queue/models.py`
    - Import works: `python -c "from queue.manager import QueueManager; from queue.models import SyncJob"`
  </verify>
  <done>
    - QueueManager initializes SQLiteAckQueue with correct parameters
    - SyncJob provides dict-like job structure
    - Directory creation handles missing parent directories
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement queue operations</name>
  <files>
    queue/operations.py
  </files>
  <action>
Create queue/operations.py with these functions (stateless, operate on passed queue):

**enqueue(queue, scene_id: int, update_type: str, data: dict) -> dict:**
- Create SyncJob dict with scene_id, update_type, data, enqueued_at=time.time()
- Add job_key for deduplication: f"scene_{scene_id}"
- Call queue.put(job)
- Return the job dict
- Log: "Enqueued sync job for scene {scene_id}"

**get_pending(queue, timeout: float = 0) -> Optional[dict]:**
- Call queue.get(timeout=timeout)
- Return job dict or None if timeout
- Note: persist-queue adds 'pqid' field automatically

**ack_job(queue, job: dict):**
- Call queue.ack(job)
- Log: "Job {pqid} completed"

**nack_job(queue, job: dict):**
- Call queue.nack(job)
- Log: "Job {pqid} returned to queue for retry"

**fail_job(queue, job: dict):**
- Call queue.ack_failed(job)
- Log: "Job {pqid} marked as failed"

**get_stats(queue_path: str) -> dict:**
- Query SQLite directly (per RESEARCH.md pattern):
  - Connect to {queue_path}/data.db
  - SELECT status, COUNT(*) GROUP BY status
  - Map status codes: 1=pending, 2=in_progress, 5=completed, 9=failed
- Return dict: {"pending": N, "in_progress": N, "completed": N, "failed": N}

Note: Import persistqueue for type hints if needed, but operations work on the queue instance passed in.
  </action>
  <verify>
    - Python syntax check: `python -m py_compile queue/operations.py`
    - Import works: `python -c "from queue.operations import enqueue, get_pending, ack_job, nack_job, fail_job, get_stats"`
  </verify>
  <done>
    - All 6 operations implemented (enqueue, get_pending, ack_job, nack_job, fail_job, get_stats)
    - Operations use persist-queue's native acknowledgment workflow
    - get_stats queries SQLite for status counts
  </done>
</task>

</tasks>

<verification>
Run integration test:
```python
# test_queue_integration.py (manual verification)
import tempfile
import os
from queue.manager import QueueManager
from queue.operations import enqueue, get_pending, ack_job, get_stats

# Setup
with tempfile.TemporaryDirectory() as tmpdir:
    mgr = QueueManager(tmpdir)
    q = mgr.get_queue()

    # Enqueue
    job = enqueue(q, scene_id=123, update_type="metadata", data={"title": "Test"})
    assert job["scene_id"] == 123

    # Stats should show 1 pending
    stats = get_stats(os.path.join(tmpdir, "queue"))
    assert stats.get("pending", 0) >= 1

    # Dequeue
    retrieved = get_pending(q, timeout=1)
    assert retrieved["scene_id"] == 123

    # Ack
    ack_job(q, retrieved)

    # Stats should show completed
    stats = get_stats(os.path.join(tmpdir, "queue"))
    print(f"Final stats: {stats}")

    print("Queue integration test PASSED")
```
</verification>

<success_criteria>
- [ ] requirements.txt exists with persist-queue>=1.1.0
- [ ] queue/ module structure matches RESEARCH.md recommended layout
- [ ] QueueManager initializes SQLiteAckQueue with auto_commit=True, multithreading=True, auto_resume=True
- [ ] Jobs can be enqueued, retrieved, acknowledged, and failed
- [ ] get_stats returns status counts by querying SQLite directly
- [ ] All files pass Python syntax check
</success_criteria>

<output>
After completion, create `.planning/phases/01-persistent-queue-foundation/01-01-SUMMARY.md`
</output>
