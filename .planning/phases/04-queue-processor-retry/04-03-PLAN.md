---
phase: 04-queue-processor-retry
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - worker/processor.py
  - tests/test_retry_orchestration.py
autonomous: true

must_haves:
  truths:
    - "Failed jobs retry with exponential backoff delays"
    - "Retry count survives worker restart (stored in job metadata)"
    - "Circuit breaker pauses processing after consecutive failures"
    - "PlexNotFound errors use longer retry window than other transient errors"
    - "Jobs exceeding max retries move to DLQ"
  artifacts:
    - path: "worker/processor.py"
      provides: "SyncWorker with retry orchestration"
      contains: "circuit_breaker"
    - path: "tests/test_retry_orchestration.py"
      provides: "Integration tests for retry flow"
      min_lines: 60
  key_links:
    - from: "worker/processor.py"
      to: "worker/backoff.py"
      via: "calculate_delay import"
      pattern: "from worker\\.backoff import"
    - from: "worker/processor.py"
      to: "worker/circuit_breaker.py"
      via: "CircuitBreaker import"
      pattern: "from worker\\.circuit_breaker import"
    - from: "worker/processor.py"
      to: "job['retry_count']"
      via: "job metadata storage"
      pattern: "job\\[.retry_count.\\]"
---

<objective>
Integrate retry orchestration into SyncWorker with backoff delays and circuit breaker.

Purpose: This is the core of Phase 4 - making the worker resilient to Plex outages by implementing exponential backoff delays, crash-safe retry counting, and circuit breaker protection.

Output: Updated `worker/processor.py` with full retry orchestration, integration tests.
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-queue-processor-retry/04-RESEARCH.md

# Prior plan outputs (needed for imports)
@.planning/phases/04-queue-processor-retry/04-01-SUMMARY.md
@.planning/phases/04-queue-processor-retry/04-02-SUMMARY.md

# Files to modify/integrate
@worker/processor.py
@worker/backoff.py
@worker/circuit_breaker.py
@plex/exceptions.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor SyncWorker for crash-safe retry tracking</name>
  <files>worker/processor.py</files>
  <action>
Modify SyncWorker to store retry state in job metadata instead of instance variable:

1. REMOVE self._retry_counts dict from __init__

2. ADD helper methods for job metadata:
```python
def _prepare_for_retry(self, job: dict, error: Exception) -> dict:
    """Add retry metadata to job before nack."""
    from worker.backoff import calculate_delay, get_retry_params

    base, cap, max_retries = get_retry_params(error)
    retry_count = job.get('retry_count', 0) + 1
    delay = calculate_delay(retry_count - 1, base, cap)  # -1 because we just incremented

    job['retry_count'] = retry_count
    job['next_retry_at'] = time.time() + delay
    job['last_error_type'] = type(error).__name__
    return job

def _is_ready_for_retry(self, job: dict) -> bool:
    """Check if job's backoff delay has elapsed."""
    next_retry_at = job.get('next_retry_at', 0)
    return time.time() >= next_retry_at

def _get_max_retries_for_error(self, error: Exception) -> int:
    """Get max retries based on error type."""
    from worker.backoff import get_retry_params
    _, _, max_retries = get_retry_params(error)
    return max_retries
```

3. UPDATE _worker_loop to check retry readiness:
```python
def _get_next_ready_job(self, timeout: float = 10) -> Optional[dict]:
    """Get next job that's ready for processing (backoff elapsed)."""
    item = get_pending(self.queue, timeout=timeout)
    if item is None:
        return None

    # Check if backoff delay has elapsed
    if not self._is_ready_for_retry(item):
        # Put back in queue - not ready yet
        nack_job(self.queue, item)
        return None

    return item
```

4. UPDATE error handling to use job metadata:
```python
except TransientError as e:
    job = self._prepare_for_retry(item, e)
    max_retries = self._get_max_retries_for_error(e)
    retry_count = job.get('retry_count', 0)

    if retry_count >= max_retries:
        print(f"[PlexSync Worker] Job {pqid} exceeded max retries ({max_retries}), moving to DLQ")
        fail_job(self.queue, item)
        self.dlq.add(job, e, retry_count)
    else:
        delay = job.get('next_retry_at', 0) - time.time()
        print(f"[PlexSync Worker] Job {pqid} failed (attempt {retry_count}/{max_retries}), retry in {delay:.1f}s: {e}")
        nack_job(self.queue, job)  # Pass updated job with retry metadata
```

IMPORTANT: The nack_job should persist the updated job dict. Check if persist-queue's nack() supports this, or if we need to ack + re-enqueue with metadata.
  </action>
  <verify>
```bash
python -c "from worker.processor import SyncWorker; print('Import OK')"
```
  </verify>
  <done>SyncWorker stores retry_count and next_retry_at in job metadata, not in instance dict. Retry count survives restart.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate circuit breaker into worker loop</name>
  <files>worker/processor.py</files>
  <action>
Add circuit breaker to SyncWorker:

1. UPDATE __init__ to create circuit breaker:
```python
def __init__(self, queue, dlq, config, max_retries=5):
    # ... existing ...
    from worker.circuit_breaker import CircuitBreaker
    self.circuit_breaker = CircuitBreaker(
        failure_threshold=5,
        recovery_timeout=60.0,
        success_threshold=1
    )
```

2. UPDATE _worker_loop to check circuit breaker:
```python
def _worker_loop(self):
    while self.running:
        try:
            # Check circuit breaker first
            if not self.circuit_breaker.can_execute():
                print(f"[PlexSync Worker] Circuit OPEN, sleeping {self.config.poll_interval}s")
                time.sleep(self.config.poll_interval)
                continue

            # Get next ready job
            item = self._get_next_ready_job(timeout=10)
            if item is None:
                continue

            # ... process job ...

            # Record success/failure with circuit breaker
            try:
                self._process_job(item)
                ack_job(self.queue, item)
                self.circuit_breaker.record_success()
                print(f"[PlexSync Worker] Job {pqid} completed")
            except TransientError as e:
                self.circuit_breaker.record_failure()
                # ... existing retry handling ...
            except PermanentError as e:
                # Permanent errors don't count against circuit
                # ... existing DLQ handling ...
```

3. ADD circuit state logging when state changes (optional but helpful):
```python
# In record_failure handling
if self.circuit_breaker.state == CircuitState.OPEN:
    print("[PlexSync Worker] Circuit breaker OPENED - pausing processing")
```
  </action>
  <verify>
```bash
python -c "from worker.processor import SyncWorker; from worker.circuit_breaker import CircuitBreaker; print('Circuit breaker integrated')"
```
  </verify>
  <done>SyncWorker checks circuit breaker before processing. Five consecutive failures pause processing until recovery timeout.</done>
</task>

<task type="auto">
  <name>Task 3: Create integration tests for retry orchestration</name>
  <files>tests/test_retry_orchestration.py</files>
  <action>
Create `tests/test_retry_orchestration.py` with tests:

```python
"""Integration tests for retry orchestration."""
import pytest
import time
from unittest.mock import Mock, patch

class TestRetryOrchestration:
    """Test retry flow with backoff and circuit breaker."""

    def test_retry_count_stored_in_job_metadata(self):
        """Verify retry_count is stored in job dict, not instance."""
        # Create worker, simulate failure, check job has retry_count

    def test_backoff_delay_increases_with_retries(self):
        """Verify delay grows exponentially."""
        # Mock time, check next_retry_at increases

    def test_plex_not_found_uses_longer_delay(self):
        """PlexNotFound gets 30s base instead of 5s."""
        # Compare delay for PlexNotFound vs PlexTemporaryError

    def test_job_moves_to_dlq_after_max_retries(self):
        """Job goes to DLQ when retries exhausted."""
        # Fail job max_retries times, verify DLQ.add called

    def test_circuit_breaker_pauses_on_consecutive_failures(self):
        """Circuit opens after threshold failures."""
        # Fail 5 times, verify next iteration sleeps

    def test_circuit_breaker_allows_test_after_recovery(self):
        """Circuit allows one request in HALF_OPEN."""
        # Open circuit, advance time, verify can_execute True

    def test_retry_survives_worker_restart_simulation(self):
        """Job metadata persists across simulated restart."""
        # Create job with retry_count, verify it's preserved
```

Use mocks for queue operations and time.time() for deterministic tests.
  </action>
  <verify>
```bash
python -m pytest tests/test_retry_orchestration.py -v
```
  </verify>
  <done>All integration tests pass. Retry orchestration works correctly with backoff, circuit breaker, and crash-safe metadata.</done>
</task>

</tasks>

<verification>
```bash
# All tests pass
python -m pytest tests/test_retry_orchestration.py tests/test_backoff.py tests/test_circuit_breaker.py -v

# Worker imports successfully
python -c "from worker.processor import SyncWorker; print('OK')"

# Verify retry metadata flow
python -c "
from worker.processor import SyncWorker
from worker.backoff import calculate_delay, get_retry_params
from plex.exceptions import PlexNotFound, PlexTemporaryError

# Check PlexNotFound gets different params
_, _, max_not_found = get_retry_params(PlexNotFound('test'))
_, _, max_temp = get_retry_params(PlexTemporaryError('test'))
assert max_not_found > max_temp, f'PlexNotFound should have more retries: {max_not_found} vs {max_temp}'
print('Retry params correct')
"
```
</verification>

<success_criteria>
- [ ] SyncWorker stores retry_count in job['retry_count'], not self._retry_counts
- [ ] Jobs check next_retry_at before processing (backoff delay respected)
- [ ] Circuit breaker integrated - 5 failures pause processing
- [ ] PlexNotFound uses 12 retries with 30s base, other errors use config max_retries with 5s base
- [ ] All integration tests pass
- [ ] Worker loop gracefully handles circuit open state
</success_criteria>

<output>
After completion, create `.planning/phases/04-queue-processor-retry/04-03-SUMMARY.md`
</output>
