---
phase: 03-integration-tests
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - tests/integration/test_queue_persistence.py
  - tests/integration/test_circuit_breaker_integration.py
autonomous: true

must_haves:
  truths:
    - "Retry metadata (retry_count, next_retry_at) persists in queue across worker restart"
    - "Circuit breaker opens after 5 consecutive failures"
    - "Circuit breaker transitions to HALF_OPEN after 60s recovery timeout"
    - "Successful job in HALF_OPEN state closes circuit"
    - "Jobs exceeding max retries move to DLQ"
  artifacts:
    - path: "tests/integration/test_queue_persistence.py"
      provides: "Queue persistence and recovery tests"
      min_lines: 80
    - path: "tests/integration/test_circuit_breaker_integration.py"
      provides: "Circuit breaker behavior tests with time control"
      min_lines: 100
  key_links:
    - from: "tests/integration/test_queue_persistence.py"
      to: "worker/processor.py"
      via: "_prepare_for_retry and _requeue_with_metadata"
      pattern: "_prepare_for_retry|_requeue_with_metadata"
    - from: "tests/integration/test_circuit_breaker_integration.py"
      to: "worker/circuit_breaker.py"
      via: "CircuitBreaker state machine"
      pattern: "CircuitState|can_execute|record_failure"
---

<objective>
Create integration tests for queue persistence/recovery and circuit breaker behavior.

Purpose: Verify crash-safe retry metadata and circuit breaker state transitions work correctly.
Output: test_queue_persistence.py (retry survival) and test_circuit_breaker_integration.py (state machine).
</objective>

<execution_context>
@/Users/trekkie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/trekkie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-integration-tests/03-RESEARCH.md

# Integration fixtures from Plan 01
@tests/integration/conftest.py

# Source modules being tested
@worker/processor.py
@worker/circuit_breaker.py
@worker/backoff.py

# Existing retry tests to extend
@tests/test_retry_orchestration.py
@tests/test_circuit_breaker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create queue persistence and recovery tests</name>
  <files>tests/integration/test_queue_persistence.py</files>
  <action>
Create tests/integration/test_queue_persistence.py with tests for crash-safe retry:

```python
"""
Integration tests for queue persistence and recovery.

Tests verify crash-safe retry behavior:
- Retry metadata (retry_count, next_retry_at, last_error_type) stored in job dict
- Metadata persists in SQLiteAckQueue across worker "restart" (new instance)
- Jobs ready for retry when next_retry_at elapsed
- Jobs not ready when still in backoff delay
- Max retries exceeded moves job to DLQ

These tests use real SQLiteAckQueue (not mocked) to verify persistence.
"""

import pytest
import time
from unittest.mock import Mock, MagicMock, patch


@pytest.mark.integration
class TestRetryMetadataPersistence:
    """Tests for retry metadata stored in job dict."""

    def test_retry_count_stored_in_job(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """retry_count field added to job dict on first failure."""
        from worker.processor import SyncWorker, TransientError

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {'scene_id': 123, 'data': {'path': '/test.mp4'}, 'pqid': 1}
        error = TransientError("test failure")

        updated_job = worker._prepare_for_retry(job, error)

        assert 'retry_count' in updated_job
        assert updated_job['retry_count'] == 1

    def test_retry_count_increments(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """retry_count increments on subsequent failures."""
        from worker.processor import SyncWorker, TransientError

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {'scene_id': 123, 'data': {'path': '/test.mp4'}, 'retry_count': 2}
        error = TransientError("test failure")

        updated_job = worker._prepare_for_retry(job, error)

        assert updated_job['retry_count'] == 3

    def test_next_retry_at_stored_in_job(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """next_retry_at timestamp added to job dict."""
        from worker.processor import SyncWorker, TransientError

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {'scene_id': 123, 'data': {'path': '/test.mp4'}}
        error = TransientError("test failure")

        before = time.time()
        updated_job = worker._prepare_for_retry(job, error)
        after = time.time()

        assert 'next_retry_at' in updated_job
        assert updated_job['next_retry_at'] >= before  # In future or now

    def test_last_error_type_stored_in_job(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """last_error_type field stores exception class name."""
        from worker.processor import SyncWorker, TransientError

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {'scene_id': 123, 'data': {'path': '/test.mp4'}}
        error = TransientError("test failure")

        updated_job = worker._prepare_for_retry(job, error)

        assert updated_job['last_error_type'] == 'TransientError'


@pytest.mark.integration
class TestQueuePersistenceAcrossRestart:
    """Tests for retry metadata surviving worker restart."""

    def test_retry_metadata_survives_in_real_queue(self, real_queue, mock_dlq, mock_config, tmp_path):
        """Retry metadata persists in SQLiteAckQueue across instances."""
        from worker.processor import SyncWorker, TransientError

        # Worker 1 prepares job for retry
        worker1 = SyncWorker(
            queue=real_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {
            'scene_id': 123,
            'update_type': 'metadata',
            'data': {'path': '/test.mp4'},
            'enqueued_at': time.time(),
            'job_key': 'scene_123',
        }
        error = TransientError("test failure")

        # Prepare retry metadata
        updated_job = worker1._prepare_for_retry(job, error)

        # Put in queue (simulating requeue)
        real_queue.put(updated_job)

        # Simulate worker restart - create new queue instance with same path
        import persistqueue
        queue_path = str(tmp_path / "test_queue")
        queue2 = persistqueue.SQLiteAckQueue(queue_path, auto_resume=True)

        # Get job from "new" queue
        retrieved_job = queue2.get(timeout=1)

        # Retry metadata should be preserved
        assert retrieved_job['retry_count'] == 1
        assert 'next_retry_at' in retrieved_job
        assert retrieved_job['last_error_type'] == 'TransientError'

    def test_requeue_preserves_all_job_fields(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """_requeue_with_metadata preserves original job data plus retry metadata."""
        from worker.processor import SyncWorker

        mock_queue.put = Mock()

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {
            'scene_id': 456,
            'update_type': 'metadata',
            'data': {'path': '/test.mp4', 'title': 'Test Title'},
            'enqueued_at': 1000.0,
            'job_key': 'scene_456',
            'pqid': 1,
            # Retry metadata
            'retry_count': 2,
            'next_retry_at': 2000.0,
            'last_error_type': 'TransientError',
        }

        worker._requeue_with_metadata(job)

        # Verify new job was put with all fields
        mock_queue.put.assert_called_once()
        new_job = mock_queue.put.call_args[0][0]

        assert new_job['scene_id'] == 456
        assert new_job['update_type'] == 'metadata'
        assert new_job['data']['title'] == 'Test Title'
        assert new_job['retry_count'] == 2
        assert new_job['next_retry_at'] == 2000.0


@pytest.mark.integration
class TestRetryReadiness:
    """Tests for is_ready_for_retry logic."""

    def test_job_ready_when_delay_elapsed(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """Job is ready when next_retry_at is in the past."""
        from worker.processor import SyncWorker

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {'scene_id': 123, 'next_retry_at': time.time() - 10}  # 10s ago

        assert worker._is_ready_for_retry(job) is True

    def test_job_not_ready_when_in_backoff(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """Job not ready when next_retry_at is in the future."""
        from worker.processor import SyncWorker

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {'scene_id': 123, 'next_retry_at': time.time() + 100}  # 100s from now

        assert worker._is_ready_for_retry(job) is False

    def test_new_job_immediately_ready(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """New job without next_retry_at is immediately ready."""
        from worker.processor import SyncWorker

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {'scene_id': 123}  # No next_retry_at

        assert worker._is_ready_for_retry(job) is True


@pytest.mark.integration
class TestDLQAfterMaxRetries:
    """Tests for jobs moving to DLQ after max retries."""

    def test_standard_error_exhausts_after_5_retries(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """TransientError exhausts after 5 retries."""
        from worker.processor import SyncWorker, TransientError

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        error = TransientError("test")
        max_retries = worker._get_max_retries_for_error(error)

        assert max_retries == 5

    def test_plex_not_found_exhausts_after_12_retries(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """PlexNotFound exhausts after 12 retries."""
        from worker.processor import SyncWorker
        from plex.exceptions import PlexNotFound

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        error = PlexNotFound("item not found")
        max_retries = worker._get_max_retries_for_error(error)

        assert max_retries == 12

    def test_job_sent_to_dlq_when_retries_exceeded(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """Job with retry_count >= max_retries should go to DLQ."""
        from worker.processor import SyncWorker, TransientError

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        # Job at retry 4, will become 5 after prepare_for_retry
        job = {'scene_id': 123, 'data': {'path': '/test.mp4'}, 'retry_count': 4}
        error = TransientError("test")

        updated_job = worker._prepare_for_retry(job, error)
        max_retries = worker._get_max_retries_for_error(error)

        # Should be at max (5)
        assert updated_job['retry_count'] == 5
        assert updated_job['retry_count'] >= max_retries
```

Tests use real SQLiteAckQueue (via real_queue fixture) where needed to verify actual persistence.
  </action>
  <verify>
Run: `pytest tests/integration/test_queue_persistence.py -v`
All tests should pass.
  </verify>
  <done>
test_queue_persistence.py created with tests for:
- Retry metadata stored in job dict (retry_count, next_retry_at, last_error_type)
- Metadata persists across worker restart (real SQLiteAckQueue)
- Requeue preserves all job fields
- Retry readiness logic (delay elapsed vs in backoff)
- DLQ after max retries (5 standard, 12 for PlexNotFound)
All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create circuit breaker integration tests with time control</name>
  <files>tests/integration/test_circuit_breaker_integration.py</files>
  <action>
Create tests/integration/test_circuit_breaker_integration.py with tests for circuit breaker behavior:

```python
"""
Integration tests for circuit breaker behavior.

Tests verify circuit breaker state machine:
- CLOSED: Normal operation, executes all requests
- OPEN: After failure_threshold failures, blocks all requests
- HALF_OPEN: After recovery_timeout, allows one test request
- Transitions based on success/failure

Uses freezegun for time control to test timeout-based transitions
without waiting for real time to elapse.
"""

import pytest
import time
from unittest.mock import Mock, MagicMock, patch
from freezegun import freeze_time


@pytest.mark.integration
class TestCircuitBreakerStateTransitions:
    """Tests for circuit breaker state machine transitions."""

    def test_initial_state_is_closed(self, fresh_circuit_breaker):
        """Circuit breaker starts in CLOSED state."""
        from worker.circuit_breaker import CircuitState

        assert fresh_circuit_breaker.state == CircuitState.CLOSED
        assert fresh_circuit_breaker.can_execute() is True

    def test_opens_after_failure_threshold(self, fresh_circuit_breaker):
        """Circuit opens after 5 consecutive failures."""
        from worker.circuit_breaker import CircuitState

        # Record 4 failures - still closed
        for _ in range(4):
            fresh_circuit_breaker.record_failure()
        assert fresh_circuit_breaker.state == CircuitState.CLOSED

        # 5th failure opens circuit
        fresh_circuit_breaker.record_failure()
        assert fresh_circuit_breaker.state == CircuitState.OPEN
        assert fresh_circuit_breaker.can_execute() is False

    def test_success_resets_failure_count(self, fresh_circuit_breaker):
        """Success in CLOSED state resets failure count."""
        from worker.circuit_breaker import CircuitState

        # Build up failures
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()

        # Success resets
        fresh_circuit_breaker.record_success()

        # Need full threshold again
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()
        assert fresh_circuit_breaker.state == CircuitState.CLOSED

        # 5th (cumulative) opens
        fresh_circuit_breaker.record_failure()
        assert fresh_circuit_breaker.state == CircuitState.OPEN


@pytest.mark.integration
class TestCircuitBreakerRecoveryWithTimeControl:
    """Tests for circuit breaker recovery using freezegun."""

    @freeze_time("2026-01-01 12:00:00")
    def test_stays_open_before_timeout(self, fresh_circuit_breaker, freezer):
        """Circuit stays OPEN before recovery timeout."""
        from worker.circuit_breaker import CircuitState

        # Open the circuit
        for _ in range(5):
            fresh_circuit_breaker.record_failure()

        assert fresh_circuit_breaker.state == CircuitState.OPEN

        # Advance time 30 seconds (less than 60s timeout)
        freezer.move_to("2026-01-01 12:00:30")

        # Still OPEN
        assert fresh_circuit_breaker.state == CircuitState.OPEN
        assert fresh_circuit_breaker.can_execute() is False

    @freeze_time("2026-01-01 12:00:00")
    def test_transitions_to_half_open_after_timeout(self, fresh_circuit_breaker, freezer):
        """Circuit transitions to HALF_OPEN after 60s recovery timeout."""
        from worker.circuit_breaker import CircuitState

        # Open the circuit
        for _ in range(5):
            fresh_circuit_breaker.record_failure()

        assert fresh_circuit_breaker.state == CircuitState.OPEN

        # Advance time past 60s timeout
        freezer.move_to("2026-01-01 12:01:01")

        # Should now be HALF_OPEN
        assert fresh_circuit_breaker.state == CircuitState.HALF_OPEN
        assert fresh_circuit_breaker.can_execute() is True

    @freeze_time("2026-01-01 12:00:00")
    def test_success_in_half_open_closes_circuit(self, fresh_circuit_breaker, freezer):
        """Success in HALF_OPEN state closes the circuit."""
        from worker.circuit_breaker import CircuitState

        # Open the circuit
        for _ in range(5):
            fresh_circuit_breaker.record_failure()

        # Advance past timeout
        freezer.move_to("2026-01-01 12:01:01")
        assert fresh_circuit_breaker.state == CircuitState.HALF_OPEN

        # Record success
        fresh_circuit_breaker.record_success()

        # Circuit should close
        assert fresh_circuit_breaker.state == CircuitState.CLOSED
        assert fresh_circuit_breaker.can_execute() is True

    @freeze_time("2026-01-01 12:00:00")
    def test_failure_in_half_open_reopens_circuit(self, fresh_circuit_breaker, freezer):
        """Failure in HALF_OPEN state reopens the circuit."""
        from worker.circuit_breaker import CircuitState

        # Open the circuit
        for _ in range(5):
            fresh_circuit_breaker.record_failure()

        # Advance past timeout to HALF_OPEN
        freezer.move_to("2026-01-01 12:01:01")
        assert fresh_circuit_breaker.state == CircuitState.HALF_OPEN

        # Record failure
        fresh_circuit_breaker.record_failure()

        # Circuit should reopen
        assert fresh_circuit_breaker.state == CircuitState.OPEN
        assert fresh_circuit_breaker.can_execute() is False


@pytest.mark.integration
class TestCircuitBreakerWithWorker:
    """Tests for circuit breaker integrated with SyncWorker."""

    def test_worker_has_circuit_breaker(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """SyncWorker initializes with circuit breaker."""
        from worker.processor import SyncWorker
        from worker.circuit_breaker import CircuitBreaker

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        assert hasattr(worker, 'circuit_breaker')
        assert isinstance(worker.circuit_breaker, CircuitBreaker)

    def test_worker_circuit_opens_after_failures(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """Worker's circuit breaker opens after 5 failures."""
        from worker.processor import SyncWorker
        from worker.circuit_breaker import CircuitState

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        # Simulate 5 job failures
        for _ in range(5):
            worker.circuit_breaker.record_failure()

        assert worker.circuit_breaker.state == CircuitState.OPEN

    def test_worker_blocks_when_circuit_open(self, mock_queue, mock_dlq, mock_config, tmp_path):
        """Worker cannot execute when circuit is OPEN."""
        from worker.processor import SyncWorker
        from worker.circuit_breaker import CircuitState

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        # Open the circuit
        for _ in range(5):
            worker.circuit_breaker.record_failure()

        # Worker should check can_execute before processing
        assert worker.circuit_breaker.can_execute() is False

    @freeze_time("2026-01-01 12:00:00")
    def test_worker_resumes_after_recovery_timeout(self, mock_queue, mock_dlq, mock_config, tmp_path, freezer):
        """Worker can execute after circuit recovers."""
        from worker.processor import SyncWorker
        from worker.circuit_breaker import CircuitState

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        # Open the circuit
        for _ in range(5):
            worker.circuit_breaker.record_failure()

        assert worker.circuit_breaker.can_execute() is False

        # Advance past recovery timeout
        freezer.move_to("2026-01-01 12:01:01")

        # Worker can execute again (HALF_OPEN)
        assert worker.circuit_breaker.can_execute() is True


@pytest.mark.integration
class TestCircuitBreakerReset:
    """Tests for manual circuit breaker reset."""

    def test_reset_closes_open_circuit(self, fresh_circuit_breaker):
        """reset() closes an OPEN circuit."""
        from worker.circuit_breaker import CircuitState

        # Open circuit
        for _ in range(5):
            fresh_circuit_breaker.record_failure()

        assert fresh_circuit_breaker.state == CircuitState.OPEN

        # Reset
        fresh_circuit_breaker.reset()

        assert fresh_circuit_breaker.state == CircuitState.CLOSED
        assert fresh_circuit_breaker.can_execute() is True

    def test_reset_clears_failure_count(self, fresh_circuit_breaker):
        """reset() clears accumulated failure count."""
        from worker.circuit_breaker import CircuitState

        # Build up failures (not enough to open)
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()

        # Reset
        fresh_circuit_breaker.reset()

        # Need full threshold again
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()
        fresh_circuit_breaker.record_failure()

        # Still closed (only 4 since reset)
        assert fresh_circuit_breaker.state == CircuitState.CLOSED


@pytest.mark.integration
class TestBackoffDelayWithTimeControl:
    """Tests for exponential backoff delay calculation."""

    def test_delay_increases_with_retry_count(self):
        """Delay range increases exponentially with retry count."""
        from worker.backoff import calculate_delay

        # Get max possible delay for each retry level
        # Using seed to get deterministic results
        delay_0 = calculate_delay(retry_count=0, base=5.0, cap=80.0, jitter_seed=42)
        delay_2 = calculate_delay(retry_count=2, base=5.0, cap=80.0, jitter_seed=42)

        # Max for retry 0: 5 * 2^0 = 5
        # Max for retry 2: 5 * 2^2 = 20
        assert delay_0 <= 5.0
        assert delay_2 <= 20.0

    def test_delay_respects_cap(self):
        """Delay never exceeds cap regardless of retry count."""
        from worker.backoff import calculate_delay

        # Very high retry count
        delay = calculate_delay(retry_count=100, base=5.0, cap=80.0, jitter_seed=42)

        assert delay <= 80.0

    @freeze_time("2026-01-01 12:00:00")
    def test_next_retry_at_calculation(self, mock_queue, mock_dlq, mock_config, tmp_path, freezer):
        """next_retry_at is calculated as current time + delay."""
        from worker.processor import SyncWorker, TransientError

        worker = SyncWorker(
            queue=mock_queue,
            dlq=mock_dlq,
            config=mock_config,
            data_dir=str(tmp_path),
        )

        job = {'scene_id': 123, 'data': {'path': '/test.mp4'}}
        error = TransientError("test")

        updated_job = worker._prepare_for_retry(job, error)

        # next_retry_at should be in the future
        current_time = time.time()
        assert updated_job['next_retry_at'] >= current_time
```

Tests use freezegun's `freeze_time` decorator and `freezer` fixture for time control.
Key scenarios:
- State transitions (CLOSED -> OPEN -> HALF_OPEN -> CLOSED/OPEN)
- Recovery timeout behavior
- Integration with SyncWorker
- Manual reset functionality
- Backoff delay calculation
  </action>
  <verify>
Run: `pytest tests/integration/test_circuit_breaker_integration.py -v`
All tests should pass.
  </verify>
  <done>
test_circuit_breaker_integration.py created with tests for:
- Circuit breaker state transitions
- Recovery timeout with time control (freezegun)
- Integration with SyncWorker
- Manual reset functionality
- Backoff delay calculation
All tests pass.
  </done>
</task>

</tasks>

<verification>
```bash
# Run queue persistence tests
pytest tests/integration/test_queue_persistence.py -v

# Run circuit breaker tests
pytest tests/integration/test_circuit_breaker_integration.py -v

# Run all integration tests together
pytest -m integration tests/integration/ -v

# Verify no regressions
pytest tests/ -v

# Check overall coverage
pytest --cov-report=term-missing
```
</verification>

<success_criteria>
- test_queue_persistence.py passes all tests for crash-safe retry
- test_circuit_breaker_integration.py passes all tests with time control
- freezegun time mocking works correctly for timeout tests
- All tests marked with @pytest.mark.integration
- No regressions in existing tests
- Overall test coverage maintained above 80%
</success_criteria>

<output>
After completion, create `.planning/phases/03-integration-tests/03-03-SUMMARY.md`
</output>
